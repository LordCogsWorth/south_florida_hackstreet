import { useRef, useState, useEffect, useCallback } from 'react';
import Tesseract from 'tesseract.js';
import { GoogleGenerativeAI } from '@google/generative-ai';
import jsPDF from 'jspdf';
import html2canvas from 'html2canvas';
import { Hands } from '@mediapipe/hands';
import { Camera } from '@mediapipe/camera_utils';
import './App.css';

function App() {
  const [videoFile, setVideoFile] = useState(null);
  const [videoURL, setVideoURL] = useState(null);
  const [screenshots, setScreenshots] = useState([]);
  const [isDetecting, setIsDetecting] = useState(false);
  const [detectedText, setDetectedText] = useState('');
  const [ocrWorker, setOcrWorker] = useState(null);
  const [apiKey, setApiKey] = useState(process.env.REACT_APP_GEMINI_API_KEY || '');
  const [isUsingChatGPT, setIsUsingChatGPT] = useState(false);
  const [chatGPTResult, setChatGPTResult] = useState('');
  
  // AI Vision Analysis Results
  const [aiAnalysisResult, setAiAnalysisResult] = useState('');
  const [showAnalysisPanel, setShowAnalysisPanel] = useState(false);
  
  // Rate limiting and conversation state
  const [lastApiCall, setLastApiCall] = useState(0);
  const [rateLimitCooldown, setRateLimitCooldown] = useState(0);
  const [conversationHistory, setConversationHistory] = useState([]);
  const [userQuestion, setUserQuestion] = useState('');
  const [apiKeyStatus, setApiKeyStatus] = useState('unchecked'); // unchecked, valid, invalid
  
  // Before/After Whiteboard Capture System
  const [beforeCapture, setBeforeCapture] = useState(null);
  const [afterCapture, setAfterCapture] = useState(null);
  const [captureMode, setCaptureMode] = useState('waiting'); // waiting, before-captured, ready-for-after
  const [comparisonResult, setComparisonResult] = useState('');
  const [showComparison, setShowComparison] = useState(false);
  
  // Video Analysis System
  const [videoAnalysisResult, setVideoAnalysisResult] = useState('');
  const [isAnalyzingVideo, setIsAnalyzingVideo] = useState(false);
  const [showVideoAnalysis, setShowVideoAnalysis] = useState(false);
  const [videoFramesSampled, setVideoFramesSampled] = useState([]);
  
  // Continuous Whiteboard Monitoring System
  const [isMonitoring, setIsMonitoring] = useState(false);
  const [currentBoardState, setCurrentBoardState] = useState(null);
  const [boardHistory, setBoardHistory] = useState([]);
  const [lastChangeDetected, setLastChangeDetected] = useState(null);
  const [monitoringInterval, setMonitoringInterval] = useState(null);
  const [changeThreshold, setChangeThreshold] = useState(0.15); // 15% change threshold
  const [monitoringFrequency, setMonitoringFrequency] = useState(5000); // Check every 5 seconds
  const [showMonitoringPanel, setShowMonitoringPanel] = useState(false);
  
  // Summary and Q&A States for Whiteboard Content
  const [boardSummary, setBoardSummary] = useState('');
  const [isGeneratingSummary, setIsGeneratingSummary] = useState(false);
  const [currentQuestion, setCurrentQuestion] = useState('');
  const [questionResponse, setQuestionResponse] = useState('');
  const [isAnswering, setIsAnswering] = useState(false);

  // Live Interactive Whiteboard System
  const [isLiveMode, setIsLiveMode] = useState(false);
  const [cameraStream, setCameraStream] = useState(null);
  const [isDrawing, setIsDrawing] = useState(false);
  const [isErasing, setIsErasing] = useState(false);
  const [drawingPath, setDrawingPath] = useState([]);
  const [allPaths, setAllPaths] = useState([]);
  const [lastDrawPoint, setLastDrawPoint] = useState(null);
  const [voiceRecognition, setVoiceRecognition] = useState(null);
  const [isListening, setIsListening] = useState(false);
  const [lastVoiceCommand, setLastVoiceCommand] = useState('');
  const [liveAIResponse, setLiveAIResponse] = useState('');
  const [hands, setHands] = useState(null);
  const [camera, setCamera] = useState(null);

  // EDITH Response Control
  const [lastEdithResponse, setLastEdithResponse] = useState(0);
  const [edithCooldown, setEdithCooldown] = useState(false);

  // Intelligent Problem-Solving Analysis
  const [problemContext, setProblemContext] = useState('');
  const [isAnalyzingProblem, setIsAnalyzingProblem] = useState(false);
  const [lastAnalysisTime, setLastAnalysisTime] = useState(0);
  const [currentProblemType, setCurrentProblemType] = useState(null);
  const [problemSuggestions, setProblemSuggestions] = useState([]);
  const [mistakeHighlights, setMistakeHighlights] = useState([]);
  const [solutionSteps, setSolutionSteps] = useState([]);
  const [workingProgress, setWorkingProgress] = useState([]);
  const [realTimeAnalysis, setRealTimeAnalysis] = useState('');
  
  // Live Summary System
  const [liveSummary, setLiveSummary] = useState('');
  const [isGeneratingLiveSummary, setIsGeneratingLiveSummary] = useState(false);
  const [lastSummaryTime, setLastSummaryTime] = useState(0);

  // EDITH Drawing System
  const [edithDrawingMode, setEdithDrawingMode] = useState(false);
  const [edithDrawingQueue, setEdithDrawingQueue] = useState([]);
  const [isEdithDrawing, setIsEdithDrawing] = useState(false);
  
  // Smart Whiteboard Layout System
  const [whiteboardLayout, setWhiteboardLayout] = useState({
    currentLine: 0,
    currentColumn: 0,
    lineHeight: 50,
    columnWidth: 300,
    marginLeft: 30,
    marginTop: 50,
    maxColumns: 4,
    canvasWidth: 1280,
    canvasHeight: 720,
    usedPositions: []
  });

  // Refs for live whiteboard
  const liveVideoRef = useRef(null);
  const liveCanvasRef = useRef(null);
  const drawingCanvasRef = useRef(null);
  const handsRef = useRef(null);

  // Handle video stream playback
  useEffect(() => {
    if (cameraStream && liveVideoRef.current) {
      liveVideoRef.current.srcObject = cameraStream;
      liveVideoRef.current.play().catch(console.error);
    }
  }, [cameraStream]);
  
  // Test Gemini API key validity
  const testApiKey = async () => {
    if (!apiKey) {
      setApiKeyStatus('invalid');
      return;
    }
    
    try {
      // Test Gemini API key with a simple request
      const genAI = new GoogleGenerativeAI(apiKey);
      const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
      
      // Simple test request
      const result = await model.generateContent("Test");
      const response = await result.response;
      
      if (response.text()) {
        setApiKeyStatus('valid');
        console.log('‚úÖ Gemini API key is valid');
      } else {
        setApiKeyStatus('invalid');
        console.log('‚ùå Gemini API key is invalid');
      }
    } catch (error) {
      setApiKeyStatus('invalid');
      console.log('‚ùå Gemini API key test failed:', error);
    }
  };  // Backend integration state
  const [isUsingBackend, setIsUsingBackend] = useState(false);
  const [lectureId, setLectureId] = useState('');
  const [uploadProgress, setUploadProgress] = useState('');
  const [processingStatus, setProcessingStatus] = useState('');
  const [queryText, setQueryText] = useState('');
  const [queryResult, setQueryResult] = useState('');
  const [isQuerying, setIsQuerying] = useState(false);
  const videoRef = useRef(null);
  const canvasRef = useRef(null);

  // Initialize OCR worker with handwriting optimizations
  const initializeOCR = async () => {
    if (ocrWorker) return ocrWorker;
    
    try {
      console.log('Starting OCR initialization...');
      const worker = await Tesseract.createWorker('eng', 1, {
        logger: m => console.log('OCR Logger:', m)
      });
      
      // Configure for better handwriting recognition
      await worker.setParameters({
        tessedit_char_whitelist: 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789.,!?()[]{}=-+*/\'"@#$%^&_|\\~ ',
        tessedit_pageseg_mode: '6', // Uniform block of text
        preserve_interword_spaces: '1',
        tessedit_do_invert: '0'
      });
      
      console.log('OCR worker created successfully');
      setOcrWorker(worker);
      return worker;
    } catch (error) {
      console.error('OCR initialization failed:', error);
      setDetectedText('OCR initialization failed: ' + error.message);
      return null;
    }
  };

  // Before/After Whiteboard Capture Functions
  const captureBeforeState = async () => {
    if (!videoRef.current || !canvasRef.current) {
      alert('Video not loaded. Please load a video first.');
      return;
    }

    const video = videoRef.current;
    const canvas = canvasRef.current;
    const ctx = canvas.getContext('2d');

    // Set canvas size to match video
    canvas.width = video.videoWidth || 640;
    canvas.height = video.videoHeight || 480;
    
    if (video.videoWidth === 0 || video.videoHeight === 0) {
      alert('Video not loaded properly');
      return;
    }

    // Capture the current frame
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    const dataURL = canvas.toDataURL('image/png');
    
    setBeforeCapture({
      image: dataURL,
      timestamp: new Date().toLocaleString(),
      frameTime: video.currentTime
    });
    
    setCaptureMode('before-captured');
    setDetectedText('üì∏ Before state captured! Make your edits to the whiteboard, then capture the after state.');
  };

  const captureAfterState = async () => {
    if (!videoRef.current || !canvasRef.current) {
      alert('Video not loaded. Please load a video first.');
      return;
    }

    if (!beforeCapture) {
      alert('Please capture the before state first.');
      return;
    }

    const video = videoRef.current;
    const canvas = canvasRef.current;
    const ctx = canvas.getContext('2d');

    // Set canvas size to match video
    canvas.width = video.videoWidth || 640;
    canvas.height = video.videoHeight || 480;

    // Capture the current frame
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    const dataURL = canvas.toDataURL('image/png');
    
    setAfterCapture({
      image: dataURL,
      timestamp: new Date().toLocaleString(),
      frameTime: video.currentTime
    });
    
    setCaptureMode('ready-for-after');
    setShowComparison(true);
    setDetectedText('üì∏ After state captured! Scroll down to see before/after comparison.');
    
    // Auto-generate comparison analysis
    await analyzeChanges();
  };

  const analyzeChanges = async () => {
    if (!beforeCapture || !afterCapture) return;
    
    setComparisonResult('üîç Analyzing changes between before and after states...');
    
    try {
      // For now, provide a basic comparison message
      // In the future, this could use AI to detect actual differences
      const timeDiff = afterCapture.frameTime - beforeCapture.frameTime;
      const timeFormatted = Math.abs(timeDiff).toFixed(1);
      
      const basicAnalysis = `üìä **Before/After Analysis Complete**

‚è±Ô∏è **Time Difference**: ${timeFormatted} seconds between captures
üìÖ **Before**: ${beforeCapture.timestamp}
üìÖ **After**: ${afterCapture.timestamp}

üîç **Visual Comparison**: 
- Both states have been captured for manual comparison
- Review the images below to identify changes
- Look for added text, erased content, or modified diagrams

üí° **Tip**: Use the AI analysis feature on each image individually to get detailed descriptions of what changed.`;

      setComparisonResult(basicAnalysis);
      
    } catch (error) {
      setComparisonResult(`‚ùå Error analyzing changes: ${error.message}`);
    }
  };

  const resetCaptures = () => {
    setBeforeCapture(null);
    setAfterCapture(null);
    setCaptureMode('waiting');
    setComparisonResult('');
    setShowComparison(false);
    setDetectedText('üîÑ Capture system reset. Ready to capture new before/after states.');
  };

  // Comprehensive Video Analysis Function with Gemini
  const analyzeEntireVideo = async () => {
    if (!videoRef.current || !canvasRef.current) {
      alert('Video not loaded. Please load a video first.');
      return;
    }

    const video = videoRef.current;
    const canvas = canvasRef.current;
    const ctx = canvas.getContext('2d');

    if (video.duration === 0 || isNaN(video.duration)) {
      alert('Video not fully loaded. Please wait for the video to load completely.');
      return;
    }

    // Check for Gemini API key
    const geminiApiKey = process.env.REACT_APP_GEMINI_API_KEY;
    if (!geminiApiKey) {
      alert('Gemini API key not configured. Please check your environment variables.');
      return;
    }

    setIsAnalyzingVideo(true);
    setShowVideoAnalysis(true);
    setVideoFramesSampled([]);
    
    const initialMessage = `ü§ñ **Starting Comprehensive Video Analysis with Gemini**

üìπ **Video Details:**
- Duration: ${Math.round(video.duration)} seconds
- Dimensions: ${video.videoWidth}x${video.videoHeight}

üîç **Analysis Process:**
1. Sampling key frames throughout the video
2. Processing images with Gemini Vision AI
3. Identifying patterns and changes over time
4. Generating comprehensive summary and insights

‚ö° **Processing Status:** Sampling frames...`;

    setVideoAnalysisResult(initialMessage);

    try {
      // Sample frames at strategic intervals throughout the video
      const framesToSample = Math.min(8, Math.max(3, Math.floor(video.duration / 10))); // 3-8 frames depending on video length
      const frameInterval = video.duration / framesToSample;
      const sampledFrames = [];

      canvas.width = video.videoWidth || 640;
      canvas.height = video.videoHeight || 480;

      // Sample frames at different timestamps
      for (let i = 0; i < framesToSample; i++) {
        const timestamp = i * frameInterval;
        
        // Seek to timestamp and capture frame
        await new Promise((resolve) => {
          const seekHandler = () => {
            video.removeEventListener('seeked', seekHandler);
            
            // Capture the frame
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
            const dataURL = canvas.toDataURL('image/jpeg', 0.8);
            
            sampledFrames.push({
              timestamp: timestamp,
              image: dataURL,
              frameNumber: i + 1
            });
            
            resolve();
          };
          
          video.addEventListener('seeked', seekHandler);
          video.currentTime = timestamp;
        });

        // Update progress
        setVideoAnalysisResult(prev => prev + `\nüì∏ Frame ${i + 1}/${framesToSample} captured at ${timestamp.toFixed(1)}s`);
      }

      setVideoFramesSampled(sampledFrames);

      // Update status
      setVideoAnalysisResult(prev => prev + '\n\nüöÄ Sending frames to Gemini Vision AI for analysis...');

      // Initialize Gemini
      const genAI = new GoogleGenerativeAI(geminiApiKey);
      const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

      // Create comprehensive analysis prompt
      const analysisPrompt = `Please provide a comprehensive analysis of this video based on the ${sampledFrames.length} key frames I've sampled throughout its ${Math.round(video.duration)}-second duration.

üéØ **COMPREHENSIVE VIDEO ANALYSIS REQUEST:**

üìä **OVERALL SUMMARY:**
- What is the main purpose/topic of this video?
- What type of content is being presented? (lecture, tutorial, meeting, etc.)
- Who is the target audience?

üîç **DETAILED CONTENT ANALYSIS:**
- What are the key concepts, topics, or subjects discussed?
- Are there any educational elements, teaching materials, or learning objectives?
- What visual aids are used? (whiteboards, slides, demonstrations, etc.)

üìù **TEXT AND WRITING ANALYSIS:**
- Extract and transcribe ALL visible text throughout the video
- Identify any equations, formulas, diagrams, or technical content
- Note any handwritten vs. printed text
- Track how text content changes over time

üë• **PEOPLE AND ACTIVITIES:**
- How many people are visible and what are their roles?
- What activities are taking place? (teaching, presenting, discussing, etc.)
- How do people interact with materials or each other?
- What gestures, movements, or behaviors are significant?

üéì **EDUCATIONAL VALUE:**
- What skills or knowledge does this video teach?
- What are the main learning outcomes?
- How is information structured and presented?
- What teaching methods or techniques are used?

‚è±Ô∏è **TEMPORAL ANALYSIS:**
- How does the content progress over time?
- Are there distinct sections or phases in the video?
- What changes occur between the beginning and end?
- Identify any pivotal moments or transitions

üèõÔ∏è **ENVIRONMENT & CONTEXT:**
- Describe the setting/location (classroom, office, lab, etc.)
- What equipment, tools, or materials are present?
- How does the environment support the content?

üí° **KEY INSIGHTS & TAKEAWAYS:**
- What are the most important points or messages?
- What would someone learn from watching this video?
- How could this content be applied or used?

üìã **STRUCTURED SUMMARY:**
Provide a clear, organized summary that someone could use to understand the video's content without watching it.

Please be thorough and detailed - I want to understand everything that happens in this video and its educational/informational value.`;

      // Convert image data URLs to parts for Gemini
      const imageParts = [];
      for (const frame of sampledFrames) {
        // Convert data URL to base64
        const base64Data = frame.image.split(',')[1];
        imageParts.push({
          inlineData: {
            data: base64Data,
            mimeType: "image/jpeg"
          }
        });
      }

      // Send to Gemini
      const result = await model.generateContent([
        analysisPrompt,
        ...imageParts
      ]);

      const response = await result.response;
      const analysis = response.text();

      const finalMessage = `‚úÖ **Comprehensive Video Analysis Complete!**

üéØ **Analysis Summary:**
- Analyzed ${sampledFrames.length} key frames from ${Math.round(video.duration)}-second video
- Processed with Gemini Vision AI
- Comprehensive content analysis complete

ÔøΩ **Detailed Analysis Results:**

${analysis}

---
*Analysis completed by Google Gemini Vision AI*
*${sampledFrames.length} frames analyzed from ${Math.round(video.duration)}-second video*`;

      setVideoAnalysisResult(finalMessage);
      setDetectedText(`ü§ñ Comprehensive video analysis complete! View detailed results in the Video Analysis panel.`);

    } catch (error) {
      console.error('Video analysis error:', error);
      const errorMsg = `‚ùå **Video Analysis Error**

**Error**: ${error.message}

**Troubleshooting:**
‚Ä¢ Ensure video is fully loaded
‚Ä¢ Check internet connection
‚Ä¢ Verify Gemini API key is correct
‚Ä¢ Try analyzing a shorter video segment

**Alternative**: Use the single frame analysis for specific moments in the video.`;
      
      setVideoAnalysisResult(errorMsg);
    } finally {
      setIsAnalyzingVideo(false);
    }
  };

  // Handle video file upload
  const handleFileUpload = async (event) => {
    const file = event.target.files[0];
    if (file && file.type.startsWith('video/')) {
      setVideoFile(file);
      const url = URL.createObjectURL(file);
      setVideoURL(url);
      setScreenshots([]);
      setDetectedText('');
      setOcrWorker(null); // Reset worker for new video
      setLectureId('');
      setProcessingStatus('');
      setQueryResult('');
      
      // If using backend, upload and process the video
      if (isUsingBackend) {
        try {
          setUploadProgress('Uploading video to backend...');
          const uploadResult = await uploadVideoToBackend(file);
          console.log('Upload result:', uploadResult);
          
          setUploadProgress('Upload complete! Processing video...');
          const processResult = await processVideoWithBackend(
            uploadResult.video.path, 
            file.name
          );
          console.log('Process result:', processResult);
          
          setLectureId(processResult.lectureId);
          setProcessingStatus(`Processing started. Lecture ID: ${processResult.lectureId}`);
          setUploadProgress('');
        } catch (error) {
          console.error('Backend upload/processing failed:', error);
          setUploadProgress(`Error: ${error.message}`);
        }
      } else {
        // Initialize OCR for local processing
        setTimeout(() => {
          initializeOCR();
        }, 1000);
      }
    } else {
      alert('Please select a valid video file');
    }
  };

  // Continuous Whiteboard Monitoring System
  
  // Compare two images to detect changes
  const compareImages = (img1DataURL, img2DataURL) => {
    return new Promise((resolve) => {
      const canvas1 = document.createElement('canvas');
      const canvas2 = document.createElement('canvas');
      const ctx1 = canvas1.getContext('2d');
      const ctx2 = canvas2.getContext('2d');
      
      const image1 = new Image();
      const image2 = new Image();
      
      let loadedImages = 0;
      
      const processComparison = () => {
        loadedImages++;
        if (loadedImages === 2) {
          // Set canvas dimensions
          const width = Math.min(image1.width, image2.width, 640);
          const height = Math.min(image1.height, image2.height, 480);
          
          canvas1.width = canvas2.width = width;
          canvas1.height = canvas2.height = height;
          
          // Draw images
          ctx1.drawImage(image1, 0, 0, width, height);
          ctx2.drawImage(image2, 0, 0, width, height);
          
          // Get image data
          const data1 = ctx1.getImageData(0, 0, width, height);
          const data2 = ctx2.getImageData(0, 0, width, height);
          
          // Calculate difference
          let differentPixels = 0;
          const totalPixels = width * height;
          
          for (let i = 0; i < data1.data.length; i += 4) {
            const r1 = data1.data[i];
            const g1 = data1.data[i + 1];
            const b1 = data1.data[i + 2];
            
            const r2 = data2.data[i];
            const g2 = data2.data[i + 1];
            const b2 = data2.data[i + 2];
            
            // Calculate color difference (simple threshold)
            const diff = Math.abs(r1 - r2) + Math.abs(g1 - g2) + Math.abs(b1 - b2);
            if (diff > 30) { // Threshold for considering a pixel "different"
              differentPixels++;
            }
          }
          
          const changePercentage = (differentPixels / totalPixels) * 100;
          resolve(changePercentage);
        }
      };
      
      image1.onload = processComparison;
      image2.onload = processComparison;
      
      image1.src = img1DataURL;
      image2.src = img2DataURL;
    });
  };

  // Capture current whiteboard state
  const captureCurrentBoardState = () => {
    if (!videoRef.current || !canvasRef.current) return null;
    
    const video = videoRef.current;
    const canvas = canvasRef.current;
    const ctx = canvas.getContext('2d');
    
    canvas.width = video.videoWidth || 640;
    canvas.height = video.videoHeight || 480;
    
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    return canvas.toDataURL('image/jpeg', 0.8);
  };

  // Start continuous monitoring
  const startMonitoring = () => {
    if (isMonitoring) return;
    
    setIsMonitoring(true);
    setShowMonitoringPanel(true);
    
    // Capture initial state
    const initialState = captureCurrentBoardState();
    if (initialState) {
      setCurrentBoardState(initialState);
      const timestamp = new Date();
      setBoardHistory([{
        id: Date.now(),
        image: initialState,
        timestamp: timestamp,
        description: "Initial whiteboard state",
        changes: "Monitoring started"
      }]);
    }
    
    // Set up monitoring interval
    const interval = setInterval(async () => {
      const newState = captureCurrentBoardState();
      if (!newState || !currentBoardState) return;
      
      try {
        const changePercentage = await compareImages(currentBoardState, newState);
        
        if (changePercentage > changeThreshold) {
          // Significant change detected!
          const timestamp = new Date();
          
          // Analyze the change with Gemini
          const geminiApiKey = process.env.REACT_APP_GEMINI_API_KEY;
          if (geminiApiKey) {
            try {
              const genAI = new GoogleGenerativeAI(geminiApiKey);
              const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

              const base64Data = newState.split(',')[1];
              const imagePart = {
                inlineData: {
                  data: base64Data,
                  mimeType: "image/jpeg"
                }
              };

              const prompt = `Please briefly describe what's visible on this whiteboard/board. Focus on:
- Any text or writing
- Diagrams, equations, or drawings
- Key concepts being taught
- Overall topic or subject
Keep it concise but informative for students reviewing the content.`;

              const result = await model.generateContent([prompt, imagePart]);
              const response = await result.response;
              const description = response.text();

              // Add to history with AI description
              const newHistoryItem = {
                id: Date.now(),
                image: newState,
                timestamp: timestamp,
                description: description,
                changes: `${changePercentage.toFixed(1)}% change detected`
              };

              setBoardHistory(prev => [...prev, newHistoryItem]);
              setCurrentBoardState(newState);
              setLastChangeDetected(timestamp);

            } catch (error) {
              console.error('Gemini analysis failed:', error);
              // Fallback without AI description
              const newHistoryItem = {
                id: Date.now(),
                image: newState,
                timestamp: timestamp,
                description: "Whiteboard content updated",
                changes: `${changePercentage.toFixed(1)}% change detected`
              };

              setBoardHistory(prev => [...prev, newHistoryItem]);
              setCurrentBoardState(newState);
              setLastChangeDetected(timestamp);
            }
          }
        }
      } catch (error) {
        console.error('Image comparison failed:', error);
      }
    }, monitoringFrequency);
    
    setMonitoringInterval(interval);
    setDetectedText(`üìä Whiteboard monitoring started! Checking for changes every ${monitoringFrequency/1000} seconds.`);
  };

  // Stop monitoring
  const stopMonitoring = () => {
    if (monitoringInterval) {
      clearInterval(monitoringInterval);
      setMonitoringInterval(null);
    }
    setIsMonitoring(false);
    setDetectedText('üìä Whiteboard monitoring stopped.');
  };

  // Clear monitoring history
  const clearBoardHistory = () => {
    setBoardHistory([]);
    setDetectedText('üìä Whiteboard history cleared.');
  };

  // Generate summary of all captured whiteboard content
  const generateBoardSummary = async () => {
    if (boardHistory.length === 0) {
      alert('üìù No whiteboard content captured yet. Start monitoring to capture board changes first!');
      return;
    }

    setIsGeneratingSummary(true);
    setBoardSummary('');

    try {
      const genAI = new GoogleGenerativeAI(apiKey);
      const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
      
      // Create a comprehensive content string from all captures
      const allContent = boardHistory.map((item, index) => 
        `**Capture ${index + 1}** (${item.timestamp.toLocaleString()}):\n${item.description}`
      ).join('\n\n');

      const prompt = `Please provide a comprehensive analysis of this classroom whiteboard session based on the ${boardHistory.length} captures I've taken throughout the class.

üéØ **COMPREHENSIVE WHITEBOARD SESSION ANALYSIS:**

üìä **OVERALL SESSION SUMMARY:**
- What is the main subject/topic being taught in this class?
- What type of lesson is this? (lecture, problem-solving, review, etc.)
- What grade level or course does this appear to be for?

üîç **DETAILED CONTENT ANALYSIS:**
- What are the key concepts, topics, or subjects covered?
- What educational elements and learning objectives are present?
- What types of visual aids are used? (diagrams, formulas, examples, etc.)

üìù **TEXT AND MATHEMATICAL CONTENT:**
- Extract and organize ALL formulas, equations, and mathematical expressions
- Identify any diagrams, charts, graphs, or technical drawings
- Note the progression of problem-solving or concept development
- Track how content builds upon previous material

üéì **EDUCATIONAL STRUCTURE:**
- What skills or knowledge does this lesson teach?
- What are the main learning outcomes and objectives?
- How is information structured and presented throughout the class?
- What teaching methods or techniques are demonstrated?

‚è±Ô∏è **LESSON PROGRESSION:**
- How does the content develop over the course of the class?
- Are there distinct sections, topics, or phases in the lesson?
- What changes occur from the beginning to end of class?
- Identify any pivotal moments, key explanations, or important transitions

üèõÔ∏è **CLASSROOM CONTEXT:**
- What subject area is being taught? (Math, Science, History, etc.)
- What level of complexity is the material?
- How does the content relate to broader curriculum goals?

üí° **KEY INSIGHTS & LEARNING POINTS:**
- What are the most important concepts students should remember?
- What would a student learn from this complete lesson?
- Which topics require the most attention or practice?
- What connections are made between different concepts?

üìã **STUDENT-READY SUMMARY:**
Provide a clear, organized study guide that students could use to review the entire lesson, including:
- Main topics covered
- Important formulas or key facts
- Examples worked through
- Concepts to remember for exams

**Captured Whiteboard Content:**
${allContent}

Please be thorough and educational - create a summary that helps students understand and review everything taught in this class session.`;

      const result = await model.generateContent(prompt);
      const response = await result.response;
      const summary = response.text();

      setBoardSummary(summary);
      setDetectedText('üìã Whiteboard session summary generated successfully!');
    } catch (error) {
      console.error('Error generating summary:', error);
      setDetectedText('‚ùå Error generating summary. Please try again.');
    } finally {
      setIsGeneratingSummary(false);
    }
  };

  // Generate PDF study guide from whiteboard summary
  const generatePDFStudyGuide = async () => {
    if (!boardSummary) {
      alert('üìö Please generate a summary first before creating a PDF study guide!');
      return;
    }

    try {
      setDetectedText('üìÑ Generating PDF study guide...');

      // Create a temporary div for PDF content
      const tempDiv = document.createElement('div');
      tempDiv.style.position = 'absolute';
      tempDiv.style.left = '-9999px';
      tempDiv.style.width = '800px';
      tempDiv.style.padding = '40px';
      tempDiv.style.fontFamily = 'Arial, sans-serif';
      tempDiv.style.fontSize = '14px';
      tempDiv.style.lineHeight = '1.6';
      tempDiv.style.color = '#333';
      tempDiv.style.backgroundColor = '#ffffff';

      // Get current date for the study guide
      const currentDate = new Date().toLocaleDateString('en-US', {
        year: 'numeric',
        month: 'long',
        day: 'numeric'
      });

      // Format the summary content for PDF
      const formattedSummary = boardSummary
        .replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>')
        .replace(/üéØ|üìä|üîç|üìù|üéì|‚è±Ô∏è|üèõÔ∏è|üí°|üìã/g, '')
        .split('\n')
        .map(line => {
          if (line.trim().startsWith('-')) {
            return `<div style="margin-left: 20px; margin-bottom: 5px;">‚Ä¢ ${line.trim().substring(1)}</div>`;
          }
          return `<div style="margin-bottom: 10px;">${line}</div>`;
        })
        .join('');

      tempDiv.innerHTML = `
        <div style="text-align: center; margin-bottom: 30px; padding-bottom: 20px; border-bottom: 2px solid #667eea;">
          <h1 style="color: #667eea; margin: 0; font-size: 28px;">üìö Whiteboard Study Guide</h1>
          <h2 style="color: #666; margin: 10px 0 0 0; font-size: 16px; font-weight: normal;">Generated on ${currentDate}</h2>
          <div style="color: #888; margin-top: 10px; font-size: 12px;">
            Based on ${boardHistory.length} whiteboard captures from classroom session
          </div>
        </div>

        <div style="margin-bottom: 20px;">
          ${formattedSummary}
        </div>

        <div style="margin-top: 40px; padding-top: 20px; border-top: 1px solid #ddd;">
          <h3 style="color: #667eea; margin-bottom: 15px;">üì∏ Captured Whiteboard Content Timeline:</h3>
          ${boardHistory.map((item, index) => `
            <div style="margin-bottom: 15px; padding: 10px; background-color: #f8f9fa; border-left: 4px solid #667eea; border-radius: 4px;">
              <div style="font-weight: bold; color: #667eea; margin-bottom: 5px;">
                Capture ${index + 1} - ${item.timestamp.toLocaleString()}
              </div>
              <div style="font-size: 12px; color: #666;">
                ${item.description}
              </div>
            </div>
          `).join('')}
        </div>

        <div style="margin-top: 30px; text-align: center; font-size: 10px; color: #888; border-top: 1px solid #ddd; padding-top: 15px;">
          Generated by Whiteboard Monitoring System | AI-Powered Educational Analysis
        </div>
      `;

      document.body.appendChild(tempDiv);

      // Convert to canvas and then PDF
      const canvas = await html2canvas(tempDiv, {
        scale: 2,
        useCORS: true,
        backgroundColor: '#ffffff',
        width: 800,
        height: tempDiv.scrollHeight
      });

      // Remove temporary div
      document.body.removeChild(tempDiv);

      // Create PDF
      const pdf = new jsPDF('p', 'mm', 'a4');
      const imgData = canvas.toDataURL('image/png');
      
      const pdfWidth = pdf.internal.pageSize.getWidth();
      const pdfHeight = pdf.internal.pageSize.getHeight();
      const imgWidth = canvas.width;
      const imgHeight = canvas.height;
      const ratio = Math.min(pdfWidth / imgWidth, pdfHeight / imgHeight);
      const imgX = (pdfWidth - imgWidth * ratio) / 2;
      const imgY = 0;

      pdf.addImage(imgData, 'PNG', imgX, imgY, imgWidth * ratio, imgHeight * ratio);

      // Generate filename with current date and time
      const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, -5);
      const filename = `Whiteboard-Study-Guide-${timestamp}.pdf`;

      // Save the PDF
      pdf.save(filename);

      setDetectedText(`üìÑ PDF study guide saved as: ${filename}`);
    } catch (error) {
      console.error('Error generating PDF:', error);
      setDetectedText('‚ùå Error generating PDF. Please try again.');
    }
  };

  // Ask questions about whiteboard content
  const askQuestionAboutBoard = async () => {
    if (!currentQuestion.trim()) {
      alert('üìù Please enter a question first!');
      return;
    }

    if (boardHistory.length === 0) {
      alert('üìù No whiteboard content available. Start monitoring to capture board changes first!');
      return;
    }

    setIsAnswering(true);
    setQuestionResponse('');

    try {
      const genAI = new GoogleGenerativeAI(apiKey);
      const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });
      
      // Create context from all captured content
      const allContent = boardHistory.map((item, index) => 
        `**Board State ${index + 1}** (${item.timestamp.toLocaleString()}):\n${item.description}`
      ).join('\n\n');

      const prompt = `
        üéì **CLASSROOM WHITEBOARD Q&A ASSISTANT**
        
        You are helping a student understand content from a classroom whiteboard session. 
        The student has a question about what was written on the board during class.

        **Complete Whiteboard Content from the Session:**
        ${allContent}

        **Student's Question:**
        "${currentQuestion}"

        **Please provide:**
        - A clear, educational answer based on the whiteboard content
        - Reference specific parts of the board content when relevant
        - If the question can't be answered from the available content, explain what information would be needed
        - Use a helpful, teaching tone appropriate for a student

        Answer the question as if you're a helpful teaching assistant who has access to everything that was written on the board during class.
      `;

      const result = await model.generateContent(prompt);
      const response = await result.response;
      const answer = response.text();

      setQuestionResponse(answer);
      setDetectedText('üí¨ Question answered based on whiteboard content!');
    } catch (error) {
      console.error('Error answering question:', error);
      setQuestionResponse('‚ùå Sorry, I encountered an error while processing your question. Please try again.');
      setDetectedText('‚ùå Error processing question. Please try again.');
    } finally {
      setIsAnswering(false);
    }
  };

  // ==================== LIVE VOICE-CONTROLLED WHITEBOARD SYSTEM ====================

  // Initialize voice-controlled mode (no camera)
  const initializeLiveMode = async () => {
    try {
      setDetectedText('üöÄ Initializing voice-controlled whiteboard...');

      // Only initialize voice recognition - no camera
      console.log('Initializing voice recognition only...');
      
      // Initialize voice recognition
      initializeVoiceRecognition();

      setIsLiveMode(true);
      setDetectedText('‚úÖ Voice-controlled whiteboard ready! Say "EDITH" followed by your command.');

    } catch (error) {
      console.error('Error initializing voice mode:', error);
      setDetectedText('‚ùå Error initializing voice mode.');
    }
  };

  // Smart Whiteboard Layout Functions
  const getNextPosition = (contentType = 'text', estimatedWidth = 200) => {
    const layout = whiteboardLayout;
    
    // Calculate position based on current line and column
    let x = layout.marginLeft + (layout.currentColumn * layout.columnWidth);
    let y = layout.marginTop + (layout.currentLine * layout.lineHeight);
    
    // Check if we need to wrap to next line (considering full canvas width)
    const maxX = layout.canvasWidth - layout.marginLeft - estimatedWidth;
    if (x > maxX) {
      // Move to next line, reset column
      const newLine = layout.currentLine + 1;
      x = layout.marginLeft;
      y = layout.marginTop + (newLine * layout.lineHeight);
      
      setWhiteboardLayout(prev => ({
        ...prev,
        currentLine: newLine,
        currentColumn: 1, // Set to 1 since we're using position 0
        usedPositions: [...prev.usedPositions, { x, y, width: estimatedWidth, height: layout.lineHeight, line: newLine, column: 0 }]
      }));
    } else {
      // Use current position, increment column
      setWhiteboardLayout(prev => ({
        ...prev,
        currentColumn: prev.currentColumn + 1,
        usedPositions: [...prev.usedPositions, { x, y, width: estimatedWidth, height: layout.lineHeight, line: prev.currentLine, column: prev.currentColumn }]
      }));
    }
    
    console.log(`üìç EDITH position: (${x}, ${y}) Line: ${layout.currentLine}, Col: ${layout.currentColumn}`);
    return { x, y };
  };

  const clearWhiteboardLayout = () => {
    setWhiteboardLayout(prev => ({
      ...prev,
      currentLine: 0,
      currentColumn: 0,
      usedPositions: []
    }));
    console.log('üó∫Ô∏è Whiteboard layout reset');
  };

  const edithClearBoard = () => {
    if (!drawingCanvasRef.current) return;
    
    const ctx = drawingCanvasRef.current.getContext('2d');
    ctx.clearRect(0, 0, drawingCanvasRef.current.width, drawingCanvasRef.current.height);
    
    // Clear user drawings too
    setDrawingPath([]);
    setAllPaths([]);
    setLastDrawPoint(null);
    
    // Reset layout
    clearWhiteboardLayout();
    
    console.log('üßπ EDITH cleared the entire whiteboard');
  };

  // EDITH Drawing Functions - Allow EDITH to draw on the camera/whiteboard
  const edithDrawText = (text, options = {}) => {
    if (!drawingCanvasRef.current) return;
    
    const ctx = drawingCanvasRef.current.getContext('2d');
    const {
      color = '#FF6B6B',
      fontSize = 24,
      fontFamily = 'Arial Bold',
      autoPosition = true
    } = options;

    // Set font to measure text width
    ctx.font = `${fontSize}px ${fontFamily}`;
    const textWidth = ctx.measureText(text).width;
    
    // Get smart position
    const { x, y } = autoPosition ? getNextPosition('text', textWidth + 20) : options;

    ctx.font = `${fontSize}px ${fontFamily}`;
    ctx.fillStyle = color;
    ctx.strokeStyle = '#FFFFFF';
    ctx.lineWidth = 3;

    // Draw text with white outline for better visibility over camera
    ctx.strokeText(text, x, y);
    ctx.fillStyle = color;
    ctx.fillText(text, x, y);
    
    console.log(`üìù EDITH wrote: "${text}" at (${x}, ${y})`);
  };

  const edithDrawShape = (shape, size, options = {}) => {
    if (!drawingCanvasRef.current) return;
    
    const ctx = drawingCanvasRef.current.getContext('2d');
    const { color = '#FF6B6B', lineWidth = 4, autoPosition = true } = options;

    // Get smart position
    const { x, y } = autoPosition ? getNextPosition('shape', size * 2) : options;

    ctx.strokeStyle = color;
    ctx.fillStyle = color;
    ctx.lineWidth = lineWidth;
    ctx.lineCap = 'round';

    switch (shape) {
      case 'circle':
        ctx.beginPath();
        ctx.arc(x + size, y - size/2, size, 0, 2 * Math.PI);
        ctx.stroke();
        break;
      case 'square':
        ctx.beginPath();
        ctx.rect(x, y - size, size, size);
        ctx.stroke();
        break;
      case 'arrow':
        // Draw arrow pointing right
        ctx.beginPath();
        ctx.moveTo(x, y - size/2);
        ctx.lineTo(x + size, y - size/2);
        ctx.moveTo(x + size - 15, y - size/2 - 10);
        ctx.lineTo(x + size, y - size/2);
        ctx.lineTo(x + size - 15, y - size/2 + 10);
        ctx.stroke();
        break;
      case 'underline':
        ctx.beginPath();
        ctx.moveTo(x, y + 5);
        ctx.lineTo(x + size, y + 5);
        ctx.stroke();
        break;
      default:
        break;
    }
    
    console.log(`üî∑ EDITH drew ${shape} at (${x}, ${y})`);
  };

  const edithDrawEquation = async (equation) => {
    if (!drawingCanvasRef.current) return;
    
    // Draw mathematical equation with proper formatting
    edithDrawText(equation, {
      color: '#FF6B6B',
      fontSize: 28,
      fontFamily: 'Times New Roman Bold',
      autoPosition: true
    });
    
    console.log(`üßÆ EDITH wrote equation: ${equation}`);
  };

  const executeEdithDrawing = async (drawingInstructions, shouldClear = false) => {
    setIsEdithDrawing(true);
    
    // Only clear if specifically requested or if board is too crowded
    if (shouldClear || whiteboardLayout.usedPositions.length > 15) {
      console.log('üßπ EDITH clearing board due to: ', shouldClear ? 'request' : 'overcrowding');
      edithClearBoard();
      await new Promise(resolve => setTimeout(resolve, 300));
    }
    
    console.log('üé® EDITH drawing:', drawingInstructions.length, 'items');
    
    for (const instruction of drawingInstructions) {
      const { type, content, options } = instruction;
      
      switch (type) {
        case 'text':
          edithDrawText(content, options);
          break;
        case 'equation':
          await edithDrawEquation(content);
          break;
        case 'shape':
          edithDrawShape(content, options?.size || 50, options);
          break;
        default:
          break;
      }
      
      // Small delay between drawing operations for visual effect
      await new Promise(resolve => setTimeout(resolve, 600));
    }
    
    console.log('‚úÖ EDITH finished drawing');
    setIsEdithDrawing(false);
  };

  // Generate Live Summary of Current Whiteboard Content
  const generateLiveSummary = useCallback(async () => {
    if (!drawingCanvasRef.current || isGeneratingLiveSummary) return;
    
    const now = Date.now();
    // Generate summary every 10 seconds if there's new content
    if (now - lastSummaryTime < 10000) return;
    
    setIsGeneratingLiveSummary(true);
    setLastSummaryTime(now);

    try {
      // Capture current drawing canvas
      const canvas = drawingCanvasRef.current;
      const imageData = canvas.toDataURL('image/png');

      const summaryPrompt = `
        üìö **LIVE WHITEBOARD SUMMARY**
        
        Analyze this live whiteboard content and provide a concise, real-time summary for students.

        **Create a brief summary that includes:**
        1. **Main Topic/Subject**: What is being worked on?
        2. **Key Content**: What important information is visible?
        3. **Progress Status**: What has been accomplished so far?
        4. **Important Notes**: Any formulas, concepts, or critical points
        5. **Next Steps**: What might come next in this work session

        **Keep it concise** (3-4 sentences max) as this is for live viewing during a session.
        **Focus on educational value** - what would a student want to remember from this?
        
        If the whiteboard is mostly empty, just say "Whiteboard is ready for new content."
      `;

      const genAI = new GoogleGenerativeAI(apiKey);
      const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

      const result = await model.generateContent({
        contents: [{
          parts: [
            { text: summaryPrompt },
            {
              inline_data: {
                mime_type: 'image/png',
                data: imageData.split(',')[1]
              }
            }
          ]
        }]
      });

      const response = await result.response;
      const summary = response.text();
      
      setLiveSummary(summary);
      console.log('‚úÖ Live summary generated:', summary.substring(0, 50) + '...');

    } catch (error) {
      console.error('‚ùå Live summary generation failed:', error);
      setLiveSummary('Unable to generate summary at this time.');
    } finally {
      setIsGeneratingLiveSummary(false);
    }
  }, [isGeneratingLiveSummary, lastSummaryTime, apiKey]);

  // Trigger summary generation when content changes
  useEffect(() => {
    if (isLiveMode && drawingPath.length > 0) {
      const timeoutId = setTimeout(() => {
        generateLiveSummary();
      }, 2000); // Generate summary 2 seconds after drawing stops

      return () => clearTimeout(timeoutId);
    }
  }, [drawingPath, isLiveMode, generateLiveSummary]);

  // Intelligent Problem Analysis - Analyzes what user is working on and provides help
  const analyzeProblemSolving = useCallback(async () => {
    if (!drawingCanvasRef.current || isAnalyzingProblem) return;
    
    const now = Date.now();
    // Analyze every 3 seconds during active drawing, or when drawing stops for 2 seconds
    const shouldAnalyze = (isDrawing && now - lastAnalysisTime > 3000) || 
                         (!isDrawing && drawingPath.length > 0 && now - lastAnalysisTime > 2000);
    
    if (!shouldAnalyze) return;

    setIsAnalyzingProblem(true);
    setLastAnalysisTime(now);

    try {
      // Capture current drawing canvas
      const canvas = drawingCanvasRef.current;
      const imageData = canvas.toDataURL('image/png');

      const analysisPrompt = `
        Analyze this handwritten work/drawing and provide intelligent assistance:

        1. **Problem Identification**: What type of problem or work is being done?
           - Mathematics (algebra, calculus, geometry, etc.)
           - Physics (mechanics, electricity, waves, etc.)
           - Chemistry (equations, structures, reactions)
           - Engineering (circuits, diagrams, calculations)
           - Programming (flowcharts, algorithms, code)
           - Other academic subjects

        2. **Current Progress Analysis**: 
           - What steps have been completed correctly?
           - What is the student currently working on?
           - Are there any visible mistakes or errors?

        3. **Intelligent Assistance**:
           - If mistakes are found, highlight them and explain why they're wrong
           - Suggest the next logical step in the solution
           - Provide hints without giving away the complete answer
           - If the work is correct, encourage and suggest verification methods

        4. **Problem-Specific Guidance**:
           - For math: Check calculations, suggest alternative methods
           - For science: Verify formulas, units, physical reasoning
           - For programming: Check logic flow, suggest optimizations

        5. **Real-time Feedback**:
           - Rate the current approach (excellent/good/needs improvement)
           - Predict what the student might struggle with next
           - Suggest resources or techniques for better understanding

        Focus on being helpful but not doing the work for them. Encourage learning and understanding.
        Be encouraging and supportive while being academically rigorous.
      `;

      const response = await fetch(
        `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${apiKey}`,
        {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
          body: JSON.stringify({
            contents: [
              {
                parts: [
                  {
                    text: analysisPrompt
                  },
                  {
                    inline_data: {
                      mime_type: 'image/png',
                      data: imageData.split(',')[1]
                    }
                  }
                ]
              }
            ],
            generationConfig: {
              temperature: 0.7,
              topP: 0.8,
              topK: 40,
              maxOutputTokens: 1000,
            }
          })
        }
      );

      if (!response.ok) throw new Error(`API Error: ${response.status}`);

      const data = await response.json();
      const analysis = data.candidates?.[0]?.content?.parts?.[0]?.text || 'Unable to analyze the current work.';
      
      setRealTimeAnalysis(analysis);
      
      // Extract problem type and suggestions from analysis
      const problemTypeMatch = analysis.match(/type of problem.*?:\s*(.*?)(?:\n|$)/i);
      if (problemTypeMatch) {
        setCurrentProblemType(problemTypeMatch[1].trim());
      }

      // Look for mistakes or errors mentioned
      const mistakePattern = /mistake|error|wrong|incorrect|fix/gi;
      if (mistakePattern.test(analysis)) {
        setMistakeHighlights(prev => [...prev, {
          timestamp: now,
          description: analysis,
          position: drawingPath.length > 0 ? drawingPath[drawingPath.length - 1] : null
        }]);
      }

      console.log('‚úÖ Problem analysis completed:', analysis.substring(0, 100) + '...');

    } catch (error) {
      console.error('‚ùå Problem analysis failed:', error);
      setRealTimeAnalysis('Unable to analyze your work right now. Keep going!');
    } finally {
      setIsAnalyzingProblem(false);
    }
  }, [drawingPath, isDrawing, isAnalyzingProblem, lastAnalysisTime, apiKey]);

  // Trigger analysis when drawing changes
  useEffect(() => {
    if (isLiveMode && drawingPath.length > 0) {
      const timeoutId = setTimeout(() => {
        analyzeProblemSolving();
      }, 1000); // Debounce analysis calls

      return () => clearTimeout(timeoutId);
    }
  }, [drawingPath, isLiveMode, analyzeProblemSolving]);

  // ==================== VOICE RECOGNITION SYSTEM ====================

  // Initialize voice recognition for "EDITH" commands
  const initializeVoiceRecognition = () => {
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      const recognition = new SpeechRecognition();
      
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'en-US';

      recognition.onstart = () => {
        setIsListening(true);
        console.log('üé§ Voice recognition started - listening for "EDITH"...');
      };

      recognition.onresult = async (event) => {
        const transcript = Array.from(event.results)
          .map(result => result[0].transcript)
          .join('');

        console.log('Heard:', transcript);

        // Check if transcript contains "EDITH"
        if (transcript.toLowerCase().includes('edith')) {
          const command = transcript.toLowerCase().replace('edith', '').trim();
          if (command) {
            setLastVoiceCommand(command);
            await processVoiceCommand(command);
          }
        }
      };

      recognition.onerror = (event) => {
        console.error('Speech recognition error:', event.error);
      };

      recognition.onend = () => {
        setIsListening(false);
        // Restart recognition to keep listening
        if (isLiveMode) {
          setTimeout(() => recognition.start(), 1000);
        }
      };

      recognition.start();
      setVoiceRecognition(recognition);
    } else {
      console.warn('Speech recognition not supported in this browser');
      setDetectedText('‚ö†Ô∏è Voice recognition not supported in this browser');
    }
  };

  // Process voice commands directed to EDITH
  const processVoiceCommand = async (command) => {
    // Prevent rapid multiple responses
    const now = Date.now();
    if (edithCooldown || now - lastEdithResponse < 5000) {
      console.log('ü§ñ EDITH is on cooldown, ignoring repeat command');
      return;
    }

    setEdithCooldown(true);
    setLastEdithResponse(now);

    try {
      setDetectedText(`üé§ EDITH heard: "${command}"`);
      
      const genAI = new GoogleGenerativeAI(apiKey);
      const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

      // Check if it's a summary request
      if (command.toLowerCase().includes('summary') || command.toLowerCase().includes('summarize')) {
        setDetectedText('üîÑ EDITH is generating a summary...');
        
        // Use the current analysis or detected text for summary
        const contentToSummarize = detectedText || liveSummary || 'No content available to summarize';
        
        const summaryPrompt = `
          üéì **EDITH - Educational Digital Intelligence Teaching Helper**
          
          Create a comprehensive educational summary of the whiteboard content.
          
          **Content to Summarize:** ${contentToSummarize}
          **Board History:** ${boardHistory.slice(-3).map(h => h.description).join(', ')}
          
          **Instructions:**
          - Provide a clear, structured summary
          - Highlight key concepts and main points
          - Include any equations, formulas, or important details
          - Organize information logically
          - Use educational language appropriate for students
          - Make it study-friendly and easy to review
          
          Format your response as a comprehensive summary that students can use for studying.
        `;

        const result = await model.generateContent(summaryPrompt);
        const summaryResponse = await result.response.text();
        
        setLiveSummary(summaryResponse);
        setDetectedText(`‚úÖ EDITH has generated a summary. Check the summary section.`);
        
      } else {
        // Handle it as a Q&A request
        setIsAnswering(true);
        
        const qaPrompt = `
          üéì **EDITH - Educational Digital Intelligence Teaching Helper**
          
          You are EDITH, an AI teaching assistant helping students with their studies.
          
          **Student's Question:** "${command}"
          **Current Context:** ${detectedText || 'General educational question'}
          **Board History:** ${boardHistory.slice(-2).map(h => h.description).join(', ')}
          
          **Instructions:**
          - Provide a clear, helpful answer to the student's question
          - If it's about the whiteboard content, reference what you know
          - Explain concepts in an educational, easy-to-understand way
          - Give examples when helpful
          - Guide the student's learning rather than just giving answers
          - Be encouraging and supportive
          - If asking about calculations, provide step-by-step explanations
          
          **Response Guidelines:**
          - Keep responses focused and relevant
          - Use appropriate academic language
          - Provide explanations, not just answers
          - Connect to broader learning concepts when possible
          
          **Tone:** Friendly, knowledgeable teaching assistant who wants to help students learn and succeed.
          
          Respond as EDITH - a helpful educational AI assistant.
        `;

        const result = await model.generateContent(qaPrompt);
        const qaResponse = await result.response.text();
        
        setQuestionResponse(qaResponse);
        setDetectedText(`‚úÖ EDITH answered: "${command}". Check the Q&A section.`);
        setIsAnswering(false);
      }

    } catch (error) {
      console.error('‚ùå Error processing EDITH command:', error);
      setDetectedText(`‚ùå EDITH encountered an error: ${error.message}`);
      setIsAnswering(false);
    } finally {
      // Reset cooldown after 3 seconds
      setTimeout(() => {
        setEdithCooldown(false);
      }, 3000);
    }
  };

  // ==================== LIVE MODE MANAGEMENT ====================
  // Stop voice-controlled mode and cleanup
  const stopLiveMode = () => {
    console.log('üõë Stopping voice-controlled mode...');
    
    // Stop voice recognition
    if (recognitionRef.current) {
      recognitionRef.current.stop();
      setIsListening(false);
    }

    // Reset states
    setIsLiveMode(false);
    setDetectedText('üì± Voice-controlled whiteboard stopped.');
    setIsDrawing(false);
    setIsErasing(false);
    setRealTimeAnalysis('');
    setMistakeHighlights([]);
  };

  // Clear all drawings
  const clearDrawings = () => {
    if (drawingCanvasRef.current) {
      const ctx = drawingCanvasRef.current.getContext('2d');
      ctx.clearRect(0, 0, drawingCanvasRef.current.width, drawingCanvasRef.current.height);
    }
    setAllPaths([]);
    setDrawingPath([]);
    setLastDrawPoint(null);
    setLiveSummary('');
    setRealTimeAnalysis('');
    setMistakeHighlights([]);
    clearWhiteboardLayout();
    console.log('üßπ User cleared the whiteboard');
  };

  // Export board history for students
  const exportBoardHistory = () => {
    const historyData = boardHistory.map(item => ({
      timestamp: item.timestamp.toISOString(),
      description: item.description,
      changes: item.changes,
      // Note: Images would need to be handled separately for full export
    }));
    
    const blob = new Blob([JSON.stringify(historyData, null, 2)], { type: 'application/json' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = `whiteboard-history-${new Date().toISOString().split('T')[0]}.json`;
    a.click();
    URL.revokeObjectURL(url);
  };

  // Take a screenshot of current video frame
  const takeScreenshot = async () => {
    if (!videoRef.current || !canvasRef.current) return;

    const video = videoRef.current;
    const canvas = canvasRef.current;
    const ctx = canvas.getContext('2d');

    // Set canvas size to match video
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    // Draw video frame to canvas
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

    // Convert to blob and create screenshot object
    canvas.toBlob((blob) => {
      const screenshot = {
        id: Date.now(),
        blob: blob,
        url: URL.createObjectURL(blob),
        timestamp: video.currentTime,
        detectedText: detectedText || 'No text detected'
      };
      setScreenshots(prev => [...prev, screenshot]);
    }, 'image/png');
  };

  // Image preprocessing for better OCR
  const preprocessImage = (canvas, ctx) => {
    const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
    const data = imageData.data;
    
    // Convert to grayscale and enhance contrast
    for (let i = 0; i < data.length; i += 4) {
      // Convert to grayscale
      const gray = Math.round(0.299 * data[i] + 0.587 * data[i + 1] + 0.114 * data[i + 2]);
      
      // Enhance contrast - make text darker and background lighter
      let enhanced;
      if (gray < 128) {
        // Dark pixels (likely text) - make darker
        enhanced = Math.max(0, gray - 30);
      } else {
        // Light pixels (likely background) - make lighter
        enhanced = Math.min(255, gray + 30);
      }
      
      // Apply threshold for better text separation
      const threshold = enhanced < 100 ? 0 : 255;
      
      data[i] = threshold;     // Red
      data[i + 1] = threshold; // Green
      data[i + 2] = threshold; // Blue
      // Alpha channel (data[i + 3]) remains unchanged
    }
    
    ctx.putImageData(imageData, 0, 0);
    return canvas;
  };

  // OCR text detection with preprocessing
  const detectText = async () => {
    if (!videoRef.current || !canvasRef.current) {
      setDetectedText('Video or canvas not available');
      return;
    }

    setIsDetecting(true);
    setDetectedText('Initializing OCR...');

    try {
      const video = videoRef.current;
      const canvas = canvasRef.current;
      const ctx = canvas.getContext('2d');

      // Set canvas size and draw current frame
      canvas.width = video.videoWidth || 640;
      canvas.height = video.videoHeight || 480;
      
      if (video.videoWidth === 0 || video.videoHeight === 0) {
        setDetectedText('Video not loaded properly');
        setIsDetecting(false);
        return;
      }

      // Draw video frame
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
      setDetectedText('Preprocessing image for handwriting...');

      // Apply image preprocessing for better OCR
      preprocessImage(canvas, ctx);

      // Get or initialize OCR worker
      let worker = ocrWorker;
      if (!worker) {
        worker = await initializeOCR();
      }

      if (worker) {
        setDetectedText('Running enhanced OCR...');
        
        // Convert processed canvas to data URL and run OCR
        const dataURL = canvas.toDataURL('image/png');
        
        const { data: { text, confidence } } = await worker.recognize(dataURL, {
          rectangle: { top: 0, left: 0, width: canvas.width, height: canvas.height }
        });
        
        const cleanText = text.trim() || 'No text detected';
        const confidenceText = confidence ? ` (Confidence: ${confidence.toFixed(1)}%)` : '';
        setDetectedText(cleanText + confidenceText);
        console.log('OCR Result:', cleanText, 'Confidence:', confidence);
      } else {
        setDetectedText('OCR worker initialization failed');
      }

    } catch (error) {
      console.error('Text detection failed:', error);
      setDetectedText('Detection failed: ' + error.message);
    } finally {
      setIsDetecting(false);
    }
  };

  // Enhanced OCR for messy handwriting
  const detectTextEnhanced = async () => {
    if (!videoRef.current || !canvasRef.current) {
      setDetectedText('Video or canvas not available');
      return;
    }

    setIsDetecting(true);
    setDetectedText('Initializing enhanced handwriting detection...');

    try {
      const video = videoRef.current;
      const canvas = canvasRef.current;
      const ctx = canvas.getContext('2d');

      // Set canvas size and draw current frame
      canvas.width = video.videoWidth || 640;
      canvas.height = video.videoHeight || 480;
      
      if (video.videoWidth === 0 || video.videoHeight === 0) {
        setDetectedText('Video not loaded properly');
        setIsDetecting(false);
        return;
      }

      // Draw video frame
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

      // Get or initialize OCR worker
      let worker = ocrWorker;
      if (!worker) {
        worker = await initializeOCR();
      }

      if (worker) {
        // Try multiple preprocessing approaches
        const results = [];
        
        // Method 1: High contrast black/white
        setDetectedText('Method 1: High contrast processing...');
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
        preprocessImage(canvas, ctx);
        const dataURL1 = canvas.toDataURL('image/png');
        
        await worker.setParameters({
          tessedit_pageseg_mode: '6', // Uniform block of text
          tessedit_char_whitelist: 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789.,!?()[]{}=-+*/\'"@#$%^&_|\\~ \n'
        });
        
        const result1 = await worker.recognize(dataURL1);
        results.push({ method: 'High Contrast', text: result1.data.text, confidence: result1.data.confidence });

        // Method 2: Single character mode for individual letters
        setDetectedText('Method 2: Character-by-character analysis...');
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
        
        await worker.setParameters({
          tessedit_pageseg_mode: '8', // Single character
          tessedit_char_whitelist: 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789'
        });
        
        const dataURL2 = canvas.toDataURL('image/png');
        const result2 = await worker.recognize(dataURL2);
        results.push({ method: 'Character Mode', text: result2.data.text, confidence: result2.data.confidence });

        // Method 3: Raw text detection with minimal processing
        setDetectedText('Method 3: Raw text detection...');
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
        
        await worker.setParameters({
          tessedit_pageseg_mode: '13', // Raw line. Treat the image as a single text line
          preserve_interword_spaces: '1'
        });
        
        const dataURL3 = canvas.toDataURL('image/png');
        const result3 = await worker.recognize(dataURL3);
        results.push({ method: 'Raw Line', text: result3.data.text, confidence: result3.data.confidence });

        // Find best result
        const bestResult = results.reduce((best, current) => 
          current.confidence > best.confidence ? current : best
        );

        // Display all results
        let displayText = `üèÜ BEST (${bestResult.method}): ${bestResult.text}\n\n`;
        results.forEach(result => {
          displayText += `${result.method} (${result.confidence?.toFixed(1)}%): ${result.text}\n\n`;
        });

        setDetectedText(displayText);
        console.log('Enhanced OCR Results:', results);
      } else {
        setDetectedText('OCR worker initialization failed');
      }

    } catch (error) {
      console.error('Enhanced text detection failed:', error);
      setDetectedText('Enhanced detection failed: ' + error.message);
    } finally {
      setIsDetecting(false);
    }
  };

  // ChatGPT integration for better text interpretation
  const enhanceWithChatGPT = async (imageDataURL, ocrText) => {
    if (!apiKey) {
      setChatGPTResult('Please enter your Gemini API key first');
      return;
    }

    setIsUsingChatGPT(true);
    setChatGPTResult('Sending to Gemini for analysis...');

    try {
      // Initialize Gemini
      const genAI = new GoogleGenerativeAI(apiKey);
      const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

      // Convert the image data URL to the format Gemini expects
      const base64Data = imageDataURL.split(',')[1];
      
      const imagePart = {
        inlineData: {
          data: base64Data,
          mimeType: "image/jpeg"
        }
      };

      const prompt = `I have handwritten text on a whiteboard that OCR is struggling to read accurately. The OCR detected this text: "${ocrText}". Please look at the image and provide a clean, corrected version of what is actually written. Focus on:
1. Correcting OCR errors and misread characters
2. Fixing spacing and formatting
3. Interpreting unclear handwriting
4. Organizing the text logically
5. Only return the corrected text content, nothing else.`;

      // Send request to Gemini
      const result = await model.generateContent([prompt, imagePart]);
      const response = await result.response;
      const correctedText = response.text();

      setChatGPTResult(correctedText);
      
      // Update the main detected text with Gemini result
      setDetectedText(`ü§ñ Gemini Enhanced:\n${correctedText}\n\nüìù Original OCR:\n${ocrText}`);
      
    } catch (error) {
      console.error('Gemini API error:', error);
      let errorMessage = `Error: ${error.message}`;
      
      if (error.message.includes('API_KEY_INVALID')) {
        errorMessage = `üîë Authentication Error: Your Gemini API key appears to be invalid. Please check your configuration.`;
      } else if (error.message.includes('RATE_LIMIT_EXCEEDED')) {
        errorMessage = `‚ö†Ô∏è Rate Limit Exceeded: You've made too many requests to the Gemini API. Please wait a few minutes and try again.`;
      }
      
      setChatGPTResult(errorMessage);
    } finally {
      setIsUsingChatGPT(false);
    }
  };

  // Gemini Scene Analysis Function
  const analyzeSceneWithAI = async (imageDataURL) => {
    const geminiApiKey = process.env.REACT_APP_GEMINI_API_KEY;
    
    if (!geminiApiKey) {
      setChatGPTResult('Gemini API key missing. Please check environment variables.');
      setAiAnalysisResult('Gemini API key missing. Please check environment variables.');
      setShowAnalysisPanel(true);
      return;
    }

    // Rate limiting check
    const now = Date.now();
    const timeSinceLastCall = now - lastApiCall;
    const minDelay = 2000; // 2 seconds between calls for Gemini
    
    if (timeSinceLastCall < minDelay) {
      const waitTime = Math.ceil((minDelay - timeSinceLastCall) / 1000);
      const waitMessage = `‚è±Ô∏è Please wait ${waitTime} seconds before making another request.`;
      setAiAnalysisResult(waitMessage);
      setShowAnalysisPanel(true);
      return;
    }

    setLastApiCall(now);
    setIsUsingChatGPT(true);
    setShowAnalysisPanel(true);

    try {
      // Show immediate feedback
      const loadingMessage = `ü§ñ **Gemini AI Analysis Starting!**

üì∏ **Image Received**: Processing your image now...
üöÄ **AI Status**: Connecting to Google Gemini Vision AI...
‚ö° **Processing**: This typically takes 5-15 seconds

**What I'm analyzing:**
‚Ä¢ All visible text (handwritten, printed, signs)
‚Ä¢ People and their activities
‚Ä¢ Objects and equipment in the scene
‚Ä¢ Educational content and concepts
‚Ä¢ Overall scene description and context

Please wait while I provide a comprehensive analysis...`;

      setChatGPTResult(loadingMessage);
      setAiAnalysisResult(loadingMessage);
      
      // Initialize Gemini
      const genAI = new GoogleGenerativeAI(geminiApiKey);
      const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

      // Prepare the prompt
      const prompt = `Please provide a comprehensive analysis of this image. I want to understand everything that's happening in this scene. Please include:

üìã **SCENE OVERVIEW:**
- What type of environment is this? (classroom, office, meeting room, etc.)
- What is the main focus or subject?

üîç **DETAILED OBSERVATIONS:**
- All text visible (handwritten, printed, on boards, signs, etc.)
- People present (count, what they're doing, clothing, gestures)
- Objects and equipment (whiteboards, computers, furniture, tools)
- Colors, lighting, and atmosphere

üìù **TEXT CONTENT:**
- Transcribe ALL visible text accurately
- Note the context of each text element
- Identify any diagrams, equations, or drawings

üéØ **EDUCATIONAL CONTENT:**
- If this appears to be educational, what subject/topic?
- Key concepts being taught or discussed
- Any visual aids or teaching materials

üìä **SUMMARY:**
- Main purpose/activity in this scene
- Most important information conveyed
- Overall assessment of what's happening

Be thorough and detailed - I want to understand everything about this scene!`;

      // Convert the image data URL to the format Gemini expects
      const base64Data = imageDataURL.split(',')[1];
      
      const imagePart = {
        inlineData: {
          data: base64Data,
          mimeType: "image/jpeg"
        }
      };

      // Send request to Gemini
      const result = await model.generateContent([prompt, imagePart]);
      const response = await result.response;
      const analysis = response.text();

      const finalMessage = `üéâ **Gemini Analysis Complete!**

${analysis}

---
*Analysis completed by Google Gemini Vision AI*`;

      setChatGPTResult(finalMessage);
      setAiAnalysisResult(finalMessage);
      setDetectedText(`ü§ñ AI Analysis Complete! Full results available in analysis panel.`);
      
    } catch (error) {
      console.error('Gemini API error:', error);
      let errorMsg = `‚ùå **Error**: ${error.message}`;
      
      if (error.message.includes('API_KEY_INVALID')) {
        errorMsg = `üîë **API Key Error**: Your Gemini API key is invalid. Please check your configuration.`;
      } else if (error.message.includes('RATE_LIMIT_EXCEEDED')) {
        errorMsg = `‚ö†Ô∏è **Rate Limit**: You've made too many requests. Please wait a moment and try again.`;
      } else if (error.message.includes('SAFETY')) {
        errorMsg = `üõ°Ô∏è **Safety Filter**: The image was blocked by safety filters. Please try a different image.`;
      } else {
        errorMsg += `

**Troubleshooting Tips:**
‚Ä¢ Check your internet connection
‚Ä¢ Verify your Gemini API key is correct
‚Ä¢ Try again in a few moments
‚Ä¢ Contact support if the issue persists`;
      }
      
      setChatGPTResult(errorMsg);
      setAiAnalysisResult(errorMsg);
      setShowAnalysisPanel(true);
    } finally {
      setIsUsingChatGPT(false);
    }
  };

  // Gemini Q&A about analyzed content
  const askQuestionAboutContent = async () => {
    const geminiApiKey = process.env.REACT_APP_GEMINI_API_KEY;
    
    if (!geminiApiKey) {
      alert('Gemini API key missing. Please check environment variables.');
      return;
    }
    
    if (!userQuestion.trim()) {
      alert('Please enter a question');
      return;
    }
    
    if (!aiAnalysisResult) {
      alert('Please analyze some content first before asking questions');
      return;
    }

    // Rate limiting check
    const now = Date.now();
    const timeSinceLastCall = now - lastApiCall;
    const minDelay = 2000; // 2 seconds for Q&A with Gemini
    
    if (timeSinceLastCall < minDelay) {
      const waitTime = Math.ceil((minDelay - timeSinceLastCall) / 1000);
      alert(`Please wait ${waitTime} seconds before asking another question.`);
      return;
    }

    setLastApiCall(now);
    setIsUsingChatGPT(true);

    try {
      // Create context-aware message for Gemini
      const contextualQuestion = `Based on my previous image analysis, please answer this question:

PREVIOUS ANALYSIS CONTEXT:
${aiAnalysisResult}

CONVERSATION HISTORY:
${conversationHistory.map(msg => `${msg.role}: ${msg.content}`).join('\n')}

CURRENT QUESTION: ${userQuestion}

Please provide a helpful, detailed answer based on the analysis and conversation context above. Be specific and reference the visual content when relevant.`;

      // Initialize Gemini
      const genAI = new GoogleGenerativeAI(geminiApiKey);
      const model = genAI.getGenerativeModel({ model: "gemini-1.5-flash" });

      // Send request to Gemini
      const result = await model.generateContent(contextualQuestion);
      const response = await result.response;
      const answer = response.text();

      // Add to conversation history
      const newConversation = [
        ...conversationHistory,
        { role: "user", content: userQuestion },
        { role: "assistant", content: answer }
      ];
      
      // Keep only last 8 messages (4 Q&A pairs) to manage context length
      const trimmedHistory = newConversation.slice(-8);
      setConversationHistory(trimmedHistory);
      
      // Clear the input
      setUserQuestion('');
      
    } catch (error) {
      console.error('Gemini API error:', error);
      let errorMessage = `Error: ${error.message}`;
      
      if (error.message.includes('API_KEY_INVALID')) {
        errorMessage = `üîë **API Key Error**: Your Gemini API key is invalid. Please check your configuration.`;
      } else if (error.message.includes('RATE_LIMIT_EXCEEDED')) {
        errorMessage = `‚ö†Ô∏è **Rate Limit**: You've made too many requests. Please wait a moment and try again.`;
      } else if (error.message.includes('SAFETY')) {
        errorMessage = `üõ°Ô∏è **Safety Filter**: Your question was blocked by safety filters. Please rephrase your question.`;
      }
      
      alert(errorMessage);
    } finally {
      setIsUsingChatGPT(false);
    }
  };

  // Full Scene Analysis - capture frame and analyze everything with AI Vision
  const analyzeCurrentScene = async () => {
    if (!videoRef.current || !canvasRef.current) {
      setDetectedText('Video or canvas not available');
      return;
    }

    setIsDetecting(true);
    setDetectedText('üì∏ Capturing frame for AI analysis...');

    try {
      const video = videoRef.current;
      const canvas = canvasRef.current;
      const ctx = canvas.getContext('2d');

      // Set canvas size and draw current frame
      canvas.width = video.videoWidth || 640;
      canvas.height = video.videoHeight || 480;
      
      if (video.videoWidth === 0 || video.videoHeight === 0) {
        setDetectedText('Video not loaded properly');
        setIsDetecting(false);
        return;
      }

      // Draw current video frame without any preprocessing for AI Vision
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
      
      // Convert to data URL for AI Vision API
      const dataURL = canvas.toDataURL('image/jpeg', 0.8); // Use JPEG for smaller size
      
      // Run AI Vision analysis
      await analyzeSceneWithAI(dataURL);
      
    } catch (error) {
      console.error('Scene analysis failed:', error);
      setDetectedText('Scene analysis failed: ' + error.message);
    } finally {
      setIsDetecting(false);
    }
  };

  // Enhanced OCR with ChatGPT integration
  const detectTextWithChatGPT = async () => {
    if (!videoRef.current || !canvasRef.current) {
      setDetectedText('Video or canvas not available');
      return;
    }

    setIsDetecting(true);
    setDetectedText('Running OCR + ChatGPT analysis...');

    try {
      const video = videoRef.current;
      const canvas = canvasRef.current;
      const ctx = canvas.getContext('2d');

      // Set canvas size and draw current frame
      canvas.width = video.videoWidth || 640;
      canvas.height = video.videoHeight || 480;
      
      if (video.videoWidth === 0 || video.videoHeight === 0) {
        setDetectedText('Video not loaded properly');
        setIsDetecting(false);
        return;
      }

      // Draw video frame
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
      setDetectedText('Preprocessing image...');

      // Apply image preprocessing for better OCR
      preprocessImage(canvas, ctx);

      // Get or initialize OCR worker
      let worker = ocrWorker;
      if (!worker) {
        worker = await initializeOCR();
      }

      if (worker) {
        setDetectedText('Running OCR...');
        
        // Convert processed canvas to data URL
        const dataURL = canvas.toDataURL('image/png');
        
        const { data: { text, confidence } } = await worker.recognize(dataURL);
        const ocrText = text.trim() || 'No text detected';
        
        setDetectedText(`OCR Result: ${ocrText}\n\nSending to ChatGPT for enhancement...`);
        
        // Send to ChatGPT for enhancement
        await enhanceWithChatGPT(dataURL, ocrText);
        
      } else {
        setDetectedText('OCR worker initialization failed');
      }

    } catch (error) {
      console.error('OCR + ChatGPT detection failed:', error);
      setDetectedText('Detection failed: ' + error.message);
    } finally {
      setIsDetecting(false);
    }
  };

  // Auto-detect text and take screenshot
  const detectAndCapture = async () => {
    await detectText();
    setTimeout(() => {
      takeScreenshot();
    }, 500); // Small delay to ensure text is updated
  };

  // Clear all screenshots
  const clearScreenshots = () => {
    screenshots.forEach(screenshot => {
      if (screenshot.url) {
        URL.revokeObjectURL(screenshot.url);
      }
    });
    setScreenshots([]);
  };

  // Backend API Integration Functions
  const API_BASE_URL = 'http://localhost:3000';

  // Upload video to backend
  const uploadVideoToBackend = async (videoFile) => {
    const formData = new FormData();
    formData.append('video', videoFile);

    try {
      const response = await fetch(`${API_BASE_URL}/upload`, {
        method: 'POST',
        body: formData
      });

      if (!response.ok) {
        throw new Error(`Upload failed: ${response.status} ${response.statusText}`);
      }

      const result = await response.json();
      return result;
    } catch (error) {
      console.error('Backend upload error:', error);
      throw error;
    }
  };

  // Process video with backend
  const processVideoWithBackend = async (videoPath, title = 'Untitled Lecture') => {
    try {
      const response = await fetch(`${API_BASE_URL}/ingest`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          videoPath,
          title
        })
      });

      if (!response.ok) {
        throw new Error(`Processing failed: ${response.status} ${response.statusText}`);
      }

      const result = await response.json();
      return result;
    } catch (error) {
      console.error('Backend processing error:', error);
      throw error;
    }
  };

  // Query processed lecture
  const queryLecture = async (lectureId, query) => {
    try {
      const response = await fetch(`${API_BASE_URL}/analyze`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          lectureId,
          query
        })
      });

      if (!response.ok) {
        throw new Error(`Query failed: ${response.status} ${response.statusText}`);
      }

      const result = await response.json();
      return result;
    } catch (error) {
      console.error('Backend query error:', error);
      throw error;
    }
  };

  // Handle querying the backend
  const handleBackendQuery = async () => {
    if (!lectureId || !queryText.trim()) {
      setQueryResult('Please ensure video is processed and enter a query');
      return;
    }

    setIsQuerying(true);
    setQueryResult('Analyzing lecture content...');

    try {
      const result = await queryLecture(lectureId, queryText.trim());
      console.log('Query result:', result);
      
      let formattedResult = `Answer: ${result.answer}\n\n`;
      
      if (result.links && result.links.length > 0) {
        formattedResult += 'Relevant timestamps:\n';
        result.links.forEach(link => {
          formattedResult += `‚Ä¢ ${link.timecode}: ${link.text}\n`;
        });
        formattedResult += '\n';
      }
      
      if (result.flashcards && result.flashcards.length > 0) {
        formattedResult += 'Generated flashcard:\n';
        result.flashcards.forEach(card => {
          formattedResult += `Q: ${card.question}\nA: ${card.answer}\n`;
        });
      }
      
      setQueryResult(formattedResult);
    } catch (error) {
      console.error('Query failed:', error);
      setQueryResult(`Query failed: ${error.message}`);
    } finally {
      setIsQuerying(false);
    }
  };

  return (
    <div className="App">
      <header className="App-header">
        <h1>üéØ AI Scene Analyzer</h1>
        <p>Upload videos and analyze everything with AI Vision + OCR text detection</p>
        
        {/* Live Interactive Whiteboard Toggle */}
        <div className="live-mode-toggle">
          <button 
            onClick={isLiveMode ? stopLiveMode : initializeLiveMode}
            className={`btn ${isLiveMode ? 'danger' : 'primary'} live-toggle`}
          >
            {isLiveMode ? 'üõë Exit Voice Mode' : 'ÔøΩ Start Voice-Controlled Whiteboard'}
          </button>
          {isListening && (
            <div className="listening-indicator">
              üé§ Listening for "EDITH"...
            </div>
          )}
        </div>
      </header>

      {/* Voice-Controlled Whiteboard Mode */}
      {isLiveMode && (
        <div className="voice-whiteboard-container">
          <div className="voice-main-area">
            <div className="voice-interface">
              <div className="voice-status">
                <h2>üé§ Voice-Controlled Whiteboard</h2>
                <p>Say "EDITH" followed by your command</p>
                
                {isListening && (
                  <div className="listening-indicator active">
                    üé§ Listening for "EDITH"...
                  </div>
                )}
                
                <div className="voice-commands-help">
                  <h3>Try saying:</h3>
                  <ul>
                    <li>"EDITH, give me a summary"</li>
                    <li>"EDITH, what is photosynthesis?"</li>
                    <li>"EDITH, explain this concept"</li>
                    <li>"EDITH, help me understand this"</li>
                  </ul>
                </div>
                
                <div className="current-analysis">
                  <h3>ÔøΩ Current Analysis</h3>
                  <div className="analysis-content">
                    {detectedText || 'No analysis available yet. Upload an image or video to get started.'}
                  </div>
                </div>
              </div>
            </div>
          </div>

          {/* AI Response Sidebar */}
          <div className="voice-sidebar">
            <div className="response-sections">
              {/* Live Summary Section */}
              <div className="summary-section">
                <h3>üìö AI Summary</h3>
                <div className="summary-content">
                  {liveSummary || 'Ask EDITH for a summary of the current content.'}
                </div>
                {liveSummary && (
                  <button 
                    onClick={generatePDFStudyGuide}
                    className="btn primary small"
                    disabled={isGeneratingPDF}
                  >
                    {isGeneratingPDF ? 'üìÑ Generating PDF...' : 'üìÑ Generate PDF Study Guide'}
                  </button>
                )}
              </div>

              {/* Q&A Section */}
              <div className="qa-section">
                <h3>üí¨ Q&A with EDITH</h3>
                <div className="qa-content">
                  {isAnswering ? (
                    <div className="loading">ü§î EDITH is thinking...</div>
                  ) : (
                    questionResponse || 'Ask EDITH any question about the content.'
                  )}
                </div>
              </div>

              {/* Voice Commands Log */}
              <div className="commands-log">
                <h3>üéôÔ∏è Recent Commands</h3>
                <div className="commands-content">
                  {lastVoiceCommand ? (
                    <div className="last-command">
                      <strong>Last:</strong> "{lastVoiceCommand}"
                    </div>
                  ) : (
                    'No commands yet.'
                  )}
                </div>
              </div>
            </div>
          </div>
        </div>
      )}

          <div className="live-sidebar">
            <div className="live-ai-assistant">
              <h3>ü§ñ EDITH - AI Teaching Assistant</h3>
              <div className="voice-status">
                <div className={`voice-indicator ${isListening ? 'listening' : ''}`}>
                  {isListening ? 'üé§ Say "EDITH" + your question' : 'üîá Voice recognition inactive'}
                </div>
                {isEdithDrawing && (
                  <div className="edith-drawing-indicator">
                    ü§ñ‚úèÔ∏è EDITH is drawing on the board...
                  </div>
                )}
              </div>
              
              {lastVoiceCommand && (
                <div className="last-command">
                  <strong>Last Command:</strong> "{lastVoiceCommand}"
                </div>
              )}
              
              {liveAIResponse && (
                <div className="live-ai-response">
                  <h4>üéì EDITH's Response:</h4>
                  <div className="response-text">
                    {liveAIResponse}
                  </div>
                </div>
              )}
              
              <div className="live-summary-section">
                <h4>üìö Live Session Notes</h4>
                <div className="live-notes">
                  <p>‚Ä¢ Point finger to draw (blue)</p>
                  <p>‚Ä¢ Open palm to erase</p>
                  <p>‚Ä¢ Ask EDITH questions by voice</p>
                  <p>‚Ä¢ Get real-time AI assistance</p>
                </div>
              </div>

              {/* Live Whiteboard Summary */}
              <div className="live-whiteboard-summary">
                <h4>üìÑ Live Content Summary</h4>
                <div className="summary-status">
                  {isGeneratingLiveSummary ? (
                    <div className="generating-summary">
                      <span className="spinner">üîÑ</span> Analyzing whiteboard content...
                    </div>
                  ) : (
                    <div className="summary-ready">
                      üìä Summary updated
                    </div>
                  )}
                </div>
                <div className="summary-content">
                  {liveSummary || 'Start drawing to see a live summary of your work...'}
                </div>
                <button 
                  onClick={generateLiveSummary} 
                  disabled={isGeneratingLiveSummary}
                  className="btn secondary small"
                >
                  üîÑ Update Summary
                </button>
              </div>

              {/* Intelligent Problem-Solving Assistant */}
              <div className="problem-solving-assistant">
                <h4>üß† Smart Problem Solver</h4>
                
                {currentProblemType && (
                  <div className="problem-type">
                    <strong>Detected:</strong> {currentProblemType}
                  </div>
                )}
                
                <div className="analysis-status">
                  {isAnalyzingProblem ? (
                    <div className="analyzing">
                      <span className="spinner">üîÑ</span> Analyzing your work...
                    </div>
                  ) : (
                    <div className="ready">
                      ‚úÖ Ready to help with your problem
                    </div>
                  )}
                </div>

                {realTimeAnalysis && (
                  <div className="real-time-feedback">
                    <h5>üí° AI Feedback:</h5>
                    <div className="feedback-content">
                      {realTimeAnalysis}
                    </div>
                  </div>
                )}

                {mistakeHighlights.length > 0 && (
                  <div className="mistake-alerts">
                    <h5>‚ö†Ô∏è Areas to Review:</h5>
                    {mistakeHighlights.slice(-3).map((mistake, index) => (
                      <div key={index} className="mistake-item">
                        <span className="mistake-time">
                          {new Date(mistake.timestamp).toLocaleTimeString()}
                        </span>
                        <span className="mistake-desc">
                          Check your work in this area
                        </span>
                      </div>
                    ))}
                  </div>
                )}

                <div className="help-hints">
                  <h5>üí™ Quick Tips:</h5>
                  <ul>
                    <li>Point your index finger to draw</li>
                    <li>Open palm to erase areas</li>
                    <li>Say "EDITH, put it on the board" for AI drawing</li>
                    <li>Ask EDITH to draw equations or diagrams</li>
                    <li>Write clearly for better analysis</li>
                  </ul>
                </div>
              </div>
            </div>
          </div>
        </div>
      )}

      <main className="App-main">
        {/* Only show traditional interface when not in live mode */}
        {!isLiveMode && (
          <>
        {/* File Upload */}
        <div className="upload-section">
          <label htmlFor="video-upload" className="upload-label">
            Choose Video File
          </label>
          <input
            id="video-upload"
            type="file"
            accept="video/*"
            onChange={handleFileUpload}
            className="file-input"
          />
          {!videoFile && (
            <div className="upload-prompt">
              <p>üìπ Select a video file to get started</p>
              <p><small>Supported formats: MP4, WebM, AVI</small></p>
            </div>
          )}
        </div>

        {/* Video Player */}
        {videoURL && (
          <div className="video-section">
            {/* API Key Input */}
            <div className="api-config">
              <h3>ü§ñ Gemini AI-Powered Analysis</h3>
              <div className="api-input-group">
                <input
                  type="password"
                  placeholder="Enter your Google Gemini API key..."
                  value={apiKey}
                  onChange={(e) => setApiKey(e.target.value)}
                  className="api-input"
                />
                <small className="api-help">
                  Get your API key from <a href="https://aistudio.google.com/app/apikey" target="_blank" rel="noopener noreferrer">Google AI Studio</a>
                  <br />
                  <strong>Features:</strong> Vision AI Analysis + Video Analysis + Q&A Chat
                </small>
              </div>
            </div>

            {/* Backend Integration Toggle */}
            <div className="backend-config">
              <h3>üöÄ Backend Analysis</h3>
              <div className="backend-toggle">
                <label className="toggle-label">
                  <input
                    type="checkbox"
                    checked={isUsingBackend}
                    onChange={(e) => setIsUsingBackend(e.target.checked)}
                  />
                  Use backend API for full lecture analysis
                </label>
                <small className="backend-help">
                  Enable this to upload videos to the backend for comprehensive analysis including board change detection and Q&A capabilities
                </small>
              </div>
              
              {/* Upload/Processing Status */}
              {uploadProgress && (
                <div className="status-message upload-status">
                  {uploadProgress}
                </div>
              )}
              
              {processingStatus && (
                <div className="status-message processing-status">
                  {processingStatus}
                </div>
              )}
              
              {/* Query Interface */}
              {lectureId && (
                <div className="query-section">
                  <h4>üîç Ask Questions About This Lecture</h4>
                  <div className="query-input-group">
                    <input
                      type="text"
                      placeholder="Ask a question about the lecture content..."
                      value={queryText}
                      onChange={(e) => setQueryText(e.target.value)}
                      className="query-input"
                      onKeyPress={(e) => e.key === 'Enter' && handleBackendQuery()}
                    />
                    <button
                      onClick={handleBackendQuery}
                      disabled={isQuerying || !queryText.trim()}
                      className="query-button"
                    >
                      {isQuerying ? 'Analyzing...' : 'Ask Question'}
                    </button>
                  </div>
                  
                  {queryResult && (
                    <div className="query-result">
                      <h5>üìã Analysis Result:</h5>
                      <pre className="result-text">{queryResult}</pre>
                    </div>
                  )}
                </div>
              )}
            </div>

            <div className="video-container">
              <video
                ref={videoRef}
                src={videoURL}
                controls
                className="video-player"
              >
                Your browser does not support the video tag.
              </video>
              <canvas ref={canvasRef} style={{ display: 'none' }} />
            </div>

            {/* Controls */}
            <div className="controls">
              <button onClick={takeScreenshot} className="btn primary">
                üì∏ Screenshot
              </button>
              <button 
                onClick={analyzeCurrentScene} 
                disabled={isDetecting || !apiKey}
                className="btn ai-vision primary-action"
              >
                {isDetecting ? 'üîÑ Analyzing with AI...' : 'üîç Analyze Full Scene with AI'}
              </button>
              <button 
                onClick={analyzeEntireVideo} 
                disabled={isAnalyzingVideo || !apiKey || !videoRef.current}
                className="btn ai-video comprehensive-analysis"
              >
                {isAnalyzingVideo ? 'üé¨ Analyzing Video...' : 'üéØ Comprehensive Video Analysis'}
              </button>
              <button 
                onClick={initializeOCR} 
                disabled={isDetecting}
                className="btn info"
              >
                üîß Initialize OCR
              </button>
              <button 
                onClick={detectAndCapture} 
                disabled={isDetecting}
                className="btn highlight"
              >
                {isDetecting ? '‚ö° Processing...' : '‚ö° Detect & Capture'}
              </button>
              {screenshots.length > 0 && (
                <button onClick={clearScreenshots} className="btn danger">
                  üóëÔ∏è Clear All ({screenshots.length})
                </button>
              )}
              <button 
                onClick={() => {
                  alert(`API Status:\n- API Key: ${apiKey ? 'Present' : 'Missing'}\n- Connection: Testing...\n\nCheck console for details.`);
                  console.log('API Key Check:', { 
                    hasKey: !!apiKey, 
                    keyLength: apiKey?.length,
                    keyStart: apiKey?.substring(0, 15) + '...',
                    timestamp: new Date().toISOString()
                  });
                }}
                className="btn info"
                title="Test API connection and debug"
              >
                üîß Test API
              </button>
            </div>

            {/* Continuous Whiteboard Monitoring Section */}
            <div className="monitoring-section">
              <h3>üîÑ Continuous Whiteboard Monitoring</h3>
              <p className="monitoring-description">
                Perfect for classrooms! Automatically captures whiteboard changes so students never miss content.
              </p>
              
              <div className="monitoring-controls">
                <button 
                  onClick={isMonitoring ? stopMonitoring : startMonitoring}
                  className={`btn ${isMonitoring ? 'danger' : 'success'} monitoring-toggle`}
                  disabled={!videoRef.current}
                >
                  {isMonitoring ? '‚èπÔ∏è Stop Monitoring' : '‚ñ∂Ô∏è Start Monitoring'}
                </button>
                
                <div className="monitoring-settings">
                  <label>
                    Check Frequency: 
                    <select 
                      value={monitoringFrequency} 
                      onChange={(e) => setMonitoringFrequency(Number(e.target.value))}
                      disabled={isMonitoring}
                    >
                      <option value={3000}>Every 3 seconds</option>
                      <option value={5000}>Every 5 seconds</option>
                      <option value={10000}>Every 10 seconds</option>
                      <option value={30000}>Every 30 seconds</option>
                    </select>
                  </label>
                  
                  <label>
                    Change Sensitivity: 
                    <select 
                      value={changeThreshold} 
                      onChange={(e) => setChangeThreshold(Number(e.target.value))}
                      disabled={isMonitoring}
                    >
                      <option value={0.05}>Very High (5%)</option>
                      <option value={0.10}>High (10%)</option>
                      <option value={0.15}>Medium (15%)</option>
                      <option value={0.25}>Low (25%)</option>
                    </select>
                  </label>
                </div>
              </div>
              
              <div className="monitoring-status">
                {isMonitoring && (
                  <div className="status-active">
                    <span className="status-indicator">üü¢</span>
                    <span>Monitoring Active - {boardHistory.length} captures</span>
                    {lastChangeDetected && (
                      <span className="last-change">
                        Last change: {lastChangeDetected.toLocaleTimeString()}
                      </span>
                    )}
                  </div>
                )}
                {!isMonitoring && boardHistory.length > 0 && (
                  <div className="status-inactive">
                    <span className="status-indicator">‚ö™</span>
                    <span>Monitoring Stopped - {boardHistory.length} captures saved</span>
                  </div>
                )}
              </div>
              
              {boardHistory.length > 0 && (
                <div className="monitoring-actions">
                  <button 
                    onClick={() => setShowMonitoringPanel(!showMonitoringPanel)}
                    className="btn info"
                  >
                    {showMonitoringPanel ? 'üìÅ Hide History' : 'üìã View History'} ({boardHistory.length})
                  </button>
                  <button 
                    onClick={generateBoardSummary}
                    className="btn primary"
                    disabled={isGeneratingSummary}
                  >
                    {isGeneratingSummary ? 'üîÑ Generating...' : 'üìö Generate Summary'}
                  </button>
                  <button 
                    onClick={exportBoardHistory}
                    className="btn secondary"
                  >
                    üì§ Export History
                  </button>
                  <button 
                    onClick={clearBoardHistory}
                    className="btn danger"
                  >
                    üóëÔ∏è Clear History
                  </button>
                </div>
              )}
            </div>

            {/* Board Summary Section */}
            {boardHistory.length > 0 && (
              <div className="summary-section">
                <h3>üìö Whiteboard Session Summary</h3>
                {boardSummary ? (
                  <div className="summary-result">
                    <div className="summary-content">
                      {boardSummary.split('\n').map((line, index) => (
                        <p key={index}>{line}</p>
                      ))}
                    </div>
                    <div className="summary-actions">
                      <button 
                        onClick={() => {
                          navigator.clipboard.writeText(boardSummary);
                          alert('üìã Summary copied to clipboard!');
                        }}
                        className="btn secondary small"
                      >
                        üìã Copy Summary
                      </button>
                      <button 
                        onClick={generatePDFStudyGuide}
                        className="btn primary small"
                      >
                        üìÑ Download PDF Study Guide
                      </button>
                      <button 
                        onClick={() => setBoardSummary('')}
                        className="btn danger small"
                      >
                        üóëÔ∏è Clear
                      </button>
                    </div>
                  </div>
                ) : (
                  <div className="summary-placeholder">
                    <p>üìù Click "Generate Summary" to get an AI-powered overview of all whiteboard content captured during this session.</p>
                    <p>Perfect for reviewing key concepts, formulas, and lesson progression!</p>
                  </div>
                )}
              </div>
            )}

            {/* Q&A Section */}
            {boardHistory.length > 0 && (
              <div className="qa-section">
                <h3>üí¨ Ask Questions About Whiteboard Content</h3>
                <div className="question-input">
                  <input 
                    type="text"
                    value={currentQuestion}
                    onChange={(e) => setCurrentQuestion(e.target.value)}
                    placeholder="Ask a question about what was written on the board..."
                    onKeyPress={(e) => e.key === 'Enter' && askQuestionAboutBoard()}
                    disabled={isAnswering}
                  />
                  <button 
                    onClick={askQuestionAboutBoard}
                    className="btn primary"
                    disabled={isAnswering || !currentQuestion.trim()}
                  >
                    {isAnswering ? 'ü§î Thinking...' : '‚ùì Ask'}
                  </button>
                </div>
                
                {questionResponse && (
                  <div className="qa-response">
                    <h4>üéì Answer:</h4>
                    <div className="response-content">
                      {questionResponse.split('\n').map((line, index) => (
                        <p key={index}>{line}</p>
                      ))}
                    </div>
                    <div className="response-actions">
                      <button 
                        onClick={() => {
                          navigator.clipboard.writeText(`Q: ${currentQuestion}\nA: ${questionResponse}`);
                          alert('üìã Q&A copied to clipboard!');
                        }}
                        className="btn secondary small"
                      >
                        üìã Copy Q&A
                      </button>
                      <button 
                        onClick={() => {
                          setQuestionResponse('');
                          setCurrentQuestion('');
                        }}
                        className="btn danger small"
                      >
                        üóëÔ∏è Clear
                      </button>
                    </div>
                  </div>
                )}
                
                {!questionResponse && (
                  <div className="qa-placeholder">
                    <p>üí° Examples of questions you can ask:</p>
                    <ul>
                      <li>"What formulas were written on the board?"</li>
                      <li>"Can you explain the diagram that was drawn?"</li>
                      <li>"What were the main points from today's lesson?"</li>
                      <li>"What examples were given for this concept?"</li>
                    </ul>
                  </div>
                )}
              </div>
            )}

            {/* Before/After Capture Section */}
            <div className="capture-section">
              <h3>üìã Before/After Whiteboard Capture</h3>
              <div className="capture-status">
                {captureMode === 'waiting' && (
                  <p>üéØ Ready to capture whiteboard states. Start by capturing the BEFORE state.</p>
                )}
                {captureMode === 'before-captured' && (
                  <p>‚úÖ Before state captured! Make your changes to the whiteboard, then capture the AFTER state.</p>
                )}
                {captureMode === 'ready-for-after' && (
                  <p>üéä Both states captured! Scroll down to view the comparison analysis.</p>
                )}
              </div>
              
              <div className="capture-buttons">
                <button 
                  onClick={captureBeforeState}
                  disabled={isDetecting}
                  className="btn secondary"
                  title="Capture the current state before making changes"
                >
                  üì∏ Capture BEFORE
                </button>
                <button 
                  onClick={captureAfterState}
                  disabled={isDetecting || !beforeCapture}
                  className="btn secondary"
                  title="Capture the state after making changes"
                >
                  üì∏ Capture AFTER
                </button>
                <button 
                  onClick={resetCaptures}
                  disabled={!beforeCapture && !afterCapture}
                  className="btn danger"
                  title="Reset and start over with new captures"
                >
                  üîÑ Reset Captures
                </button>
              </div>
            </div>

            {/* Detected Text Display */}
            <div className="text-detection">
              <h3>üîç Detected Text:</h3>
              <div className="detected-text-box">
                {detectedText || 'Click "Detect Text" to analyze the current frame'}
              </div>
            </div>
          </div>
        )}

        {/* Screenshots Gallery */}
        {/* AI Vision Analysis Panel */}
        {showAnalysisPanel && aiAnalysisResult && (
          <div className="ai-analysis-panel">
            <div className="panel-header">
              <h3>ü§ñ Gemini Vision Analysis Results</h3>
              <button 
                onClick={() => setShowAnalysisPanel(false)}
                className="close-panel-btn"
              >
                ‚úï
              </button>
            </div>
            <div className="analysis-content">
              <div className="analysis-section">
                <h4>üìä Scene Analysis</h4>
                <pre className="analysis-text">{aiAnalysisResult}</pre>
              </div>
              
              {/* AI Agent Q&A Interface */}
              <div className="qa-section">
                <h4>ü§ñ Ask Questions About This Content</h4>
                <div className="qa-input-group">
                  <input
                    type="text"
                    placeholder="Ask a question about what you see in the image..."
                    value={userQuestion}
                    onChange={(e) => setUserQuestion(e.target.value)}
                    className="qa-input"
                    onKeyPress={(e) => e.key === 'Enter' && askQuestionAboutContent()}
                    disabled={isUsingChatGPT}
                  />
                  <button
                    onClick={askQuestionAboutContent}
                    disabled={isUsingChatGPT || !userQuestion.trim()}
                    className="qa-button"
                  >
                    {isUsingChatGPT ? 'ü§î Thinking...' : 'üí¨ Ask'}
                  </button>
                </div>
                
                {/* Conversation History */}
                {conversationHistory.length > 0 && (
                  <div className="conversation-history">
                    <h5>üí≠ Conversation</h5>
                    <div className="conversation-messages">
                      {conversationHistory.map((message, index) => (
                        <div 
                          key={index} 
                          className={`message ${message.role === 'user' ? 'user-message' : 'ai-message'}`}
                        >
                          <div className="message-role">
                            {message.role === 'user' ? 'üë§ You:' : 'ü§ñ AI:'}
                          </div>
                          <div className="message-content">{message.content}</div>
                        </div>
                      ))}
                    </div>
                    <button 
                      onClick={() => setConversationHistory([])}
                      className="clear-conversation-btn"
                    >
                      üóëÔ∏è Clear Conversation
                    </button>
                  </div>
                )}
              </div>
            </div>
          </div>
        )}

        {/* Comprehensive Video Analysis Panel */}
        {showVideoAnalysis && videoAnalysisResult && (
          <div className="video-analysis-panel">
            <div className="panel-header">
              <h3>üé¨ Comprehensive Video Analysis</h3>
              <button 
                onClick={() => setShowVideoAnalysis(false)}
                className="close-panel-btn"
              >
                ‚úï
              </button>
            </div>
            <div className="analysis-content">
              <div className="analysis-section">
                <h4>üìä Complete Video Analysis</h4>
                <pre className="video-analysis-text">{videoAnalysisResult}</pre>
              </div>
              
              {/* Sampled Frames Display */}
              {videoFramesSampled.length > 0 && (
                <div className="sampled-frames-section">
                  <h4>üì∏ Analyzed Video Frames ({videoFramesSampled.length} samples)</h4>
                  <div className="frames-grid">
                    {videoFramesSampled.map((frame, index) => (
                      <div key={index} className="frame-sample">
                        <img src={frame.image} alt={`Frame ${frame.frameNumber}`} />
                        <div className="frame-info">
                          <span>Frame {frame.frameNumber}</span>
                          <span>{frame.timestamp.toFixed(1)}s</span>
                        </div>
                      </div>
                    ))}
                  </div>
                </div>
              )}
            </div>
          </div>
        )}

        {/* Whiteboard Monitoring History Panel */}
        {showMonitoringPanel && boardHistory.length > 0 && (
          <div className="monitoring-panel">
            <div className="panel-header">
              <h3>üìã Whiteboard History - {boardHistory.length} Captures</h3>
              <button 
                onClick={() => setShowMonitoringPanel(false)}
                className="close-panel-btn"
              >
                ‚úï
              </button>
            </div>
            <div className="monitoring-content">
              <div className="monitoring-stats">
                <p><strong>üìä Session Stats:</strong></p>
                <ul>
                  <li>Total Captures: {boardHistory.length}</li>
                  <li>Monitoring Status: {isMonitoring ? 'üü¢ Active' : '‚ö™ Stopped'}</li>
                  <li>Check Frequency: Every {monitoringFrequency/1000}s</li>
                  <li>Change Threshold: {(changeThreshold * 100).toFixed(0)}%</li>
                  {lastChangeDetected && <li>Last Change: {lastChangeDetected.toLocaleString()}</li>}
                </ul>
              </div>
              
              <div className="board-history-grid">
                {boardHistory.slice().reverse().map((item, index) => (
                  <div key={item.id} className="history-item">
                    <div className="history-header">
                      <span className="history-time">
                        {item.timestamp.toLocaleTimeString()}
                      </span>
                      <span className="history-change">
                        {item.changes}
                      </span>
                    </div>
                    <div className="history-image">
                      <img 
                        src={item.image} 
                        alt={`Board state at ${item.timestamp.toLocaleTimeString()}`}
                        className="board-thumbnail"
                      />
                    </div>
                    <div className="history-description">
                      <h4>üìù Content Analysis:</h4>
                      <p>{item.description}</p>
                    </div>
                    <div className="history-actions">
                      <button 
                        onClick={() => {
                          const link = document.createElement('a');
                          link.href = item.image;
                          link.download = `whiteboard-${item.timestamp.toISOString()}.jpg`;
                          link.click();
                        }}
                        className="btn small secondary"
                      >
                        üíæ Download
                      </button>
                      <button 
                        onClick={() => {
                          navigator.clipboard.writeText(item.description);
                          alert('Description copied to clipboard!');
                        }}
                        className="btn small info"
                      >
                        üìã Copy Text
                      </button>
                    </div>
                  </div>
                ))}
              </div>
              
              {boardHistory.length === 0 && (
                <div className="no-history">
                  <p>üìù No whiteboard changes captured yet.</p>
                  <p>Start monitoring to automatically capture board content changes!</p>
                </div>
              )}
            </div>
          </div>
        )}

        {/* Screenshots Gallery */}
        {screenshots.length > 0 && (
          <div className="gallery-section">
            <h3>üì± Screenshots ({screenshots.length})</h3>
            <div className="gallery">
              {screenshots.map((screenshot) => (
                <div key={screenshot.id} className="screenshot-item">
                  <img 
                    src={screenshot.url} 
                    alt={`Screenshot at ${screenshot.timestamp.toFixed(1)}s`}
                    className="screenshot-image"
                  />
                  <div className="screenshot-info">
                    <p><strong>‚è±Ô∏è Time:</strong> {screenshot.timestamp.toFixed(1)}s</p>
                    <p><strong>üìù Text:</strong> {screenshot.detectedText}</p>
                  </div>
                </div>
              ))}
            </div>
          </div>
        )}
      </main>
    </div>
  );
}

export default App;